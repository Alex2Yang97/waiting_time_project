{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(r'D:\\jupyter files\\waiting_time_project\\my_tools')\n",
    "import tools_for_os.for_df as ml_df\n",
    "import tools_for_os.for_file as ml_fl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\jupyter files\\data_waiting_time_project\\preprocess_data\\patient_duration_LSTM_data\\ already existed!\n"
     ]
    }
   ],
   "source": [
    "data_path = 'D:\\\\jupyter files\\\\data_waiting_time_project\\\\preprocess_data\\\\'\n",
    "\n",
    "pat_duration_LSTM_data_path = data_path + 'patient_duration_LSTM_data\\\\'\n",
    "ml_fl.create_folder(pat_duration_LSTM_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_part1 = pd.read_csv(data_path + 'data_part1.csv', index_col = 0)\n",
    "data_part2 = pd.read_csv(data_path + 'data_part2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把字符串转成datetime\n",
    "def str_to_Datetime(st):\n",
    "    dt = datetime.datetime.strptime(st, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num = ['Scheduled_duration', 'Actual_duration',\n",
    "               'age', 'TreatmentTime_total', 'ImagesTaken_total',\n",
    "               'MU_total', 'MUCoeff_total', 'Interval_scheduled']\n",
    "\n",
    "# RadiationId\n",
    "feature_cate = ['dxt_AliasName', 'Sex', 'AliasSerNum',\n",
    "                'month', 'week', 'hour', 'DoctorSerNum', \n",
    "                'TreatmentOrientation', 'FractionNumber',\n",
    "                'UserName', 'CourseId', 'ResourceSerNum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为需要进行onehot encoding，所以在拼接数据之前，先进行数据格式的处理\n",
    "for col in feature_cate:\n",
    "    try:\n",
    "        data_part1[col].fillna('Unknown', inplace = True)\n",
    "        data_part1[col] = data_part1[col].astype(str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        data_part2[col].fillna('Unknown', inplace = True)\n",
    "        data_part2[col] = data_part2[col].astype(str)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for col in feature_num:\n",
    "    try:\n",
    "        data_part1.fillna(0, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        data_part2.fillna(0, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# data_num = log1p(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "              n_values=None, sparse=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoder\n",
    "label_encoder_dxt_AliasName = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_dxt_AliasName.fit(data_part1.dxt_AliasName.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_Sex = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_Sex.fit(data_part1.Sex.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_AliasSerNum = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_AliasSerNum.fit(data_part1.AliasSerNum.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_month = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_month.fit(data_part1.month.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_week = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_week.fit(data_part1.week.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_hour = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_hour.fit(data_part1.hour.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_DoctorSerNum = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_DoctorSerNum.fit(data_part1.DoctorSerNum.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_TreatmentOrientation = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_TreatmentOrientation.fit(data_part1.TreatmentOrientation.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_FractionNumber = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_FractionNumber.fit(data_part2.FractionNumber.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_UserName = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_UserName.fit(data_part2.UserName.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_CourseId = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_CourseId.fit(data_part2.CourseId.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_ResourceSerNum = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_ResourceSerNum.fit(data_part2.ResourceSerNum.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorical feature 不同取值的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_num = ['Scheduled_duration', 'Actual_duration', 'Actual_duration',\n",
    "#                'age', 'TreatmentTime_total', 'ImagesTaken_total',\n",
    "#                'MU_total', 'MUCoeff_total']\n",
    "\n",
    "# # RadiationId\n",
    "# feature_cate = ['dxt_AliasName', 'Sex', 'AliasSerNum',\n",
    "#                 'month', 'week', 'hour', 'DoctorSerNum', \n",
    "#                 'TreatmentOrientation', 'FractionNumber',\n",
    "#                 'UserName', 'CourseId', 'ResourceSerNum']\n",
    "\n",
    "# feature_count1 = pd.DataFrame({})\n",
    "# for col in feature_cate:\n",
    "#     try:\n",
    "#         n = len(data_part1[col].unique())\n",
    "#         feature_count1[col] = [n]\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "# feature_count2 = pd.DataFrame({})\n",
    "# for col in feature_cate:\n",
    "#     try:\n",
    "#         n = len(data_part2[col].unique())\n",
    "#         feature_count2[col] = [n]\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69142 with one appointment\n",
      "251892 with more appointment\n"
     ]
    }
   ],
   "source": [
    "data_part1_grouped = data_part1.groupby('AppointmentSerNum')\n",
    "count_appt = data_part1_grouped.count()\n",
    "appt_one_list = count_appt[count_appt.Sex == 1].index.tolist()\n",
    "appt_more_list = count_appt[count_appt.Sex > 1].index.tolist()\n",
    "\n",
    "print(f'{len(appt_one_list)} with one appointment')\n",
    "print(f'{len(appt_more_list)} with more appointment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(x):\n",
    "    x = list(x)\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cateorical features\n",
      "\n",
      "Start PatientSerNum\n",
      "Start Sex\n",
      "Start DoctorSerNum\n",
      "Start date\n",
      "Start ScheduledStartTime\n",
      "Start ScheduledEndTime\n",
      "Start ActualStartDate\n",
      "Start ActualEndDate\n",
      "Start dxt_AliasName\n",
      "Start AliasSerNum\n",
      "Start CourseSerNum\n",
      "Start PlanSerNum\n",
      "Start TreatmentOrientation\n",
      "Start month\n",
      "Start week\n",
      "Start hour\n",
      "Start AppointmentSerNum\n",
      "Start numberical features\n",
      "\n",
      "Start age\n",
      "Start Scheduled_duration\n",
      "Start Actual_duration\n"
     ]
    }
   ],
   "source": [
    "new_appt = pd.DataFrame({})\n",
    "\n",
    "print('Start cateorical features')\n",
    "print('\\nStart PatientSerNum')\n",
    "new_appt['PatientSerNum'] = data_part1.groupby('AppointmentSerNum').PatientSerNum.apply(set)\n",
    "new_appt['PatientSerNum'] = new_appt['PatientSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start Sex')\n",
    "new_appt['Sex'] = data_part1.groupby('AppointmentSerNum').Sex.apply(set)\n",
    "new_appt['Sex'] = new_appt['Sex'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start DoctorSerNum')\n",
    "new_appt['DoctorSerNum'] = data_part1.groupby('AppointmentSerNum').DoctorSerNum.apply(set)\n",
    "new_appt['DoctorSerNum'] = new_appt['DoctorSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start date')\n",
    "new_appt['date'] = data_part1.groupby('AppointmentSerNum').date.apply(set)\n",
    "new_appt['date'] = new_appt['date'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ScheduledStartTime')\n",
    "new_appt['ScheduledStartTime'] = data_part1.groupby('AppointmentSerNum').ScheduledStartTime.apply(set)\n",
    "new_appt['ScheduledStartTime'] = new_appt['ScheduledStartTime'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ScheduledEndTime')\n",
    "new_appt['ScheduledEndTime'] = data_part1.groupby('AppointmentSerNum').ScheduledEndTime.apply(set)\n",
    "new_appt['ScheduledEndTime'] = new_appt['ScheduledEndTime'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ActualStartDate')\n",
    "new_appt['ActualStartDate'] = data_part1.groupby('AppointmentSerNum').ActualStartDate.apply(set)\n",
    "new_appt['ActualStartDate'] = new_appt['ActualStartDate'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ActualEndDate')\n",
    "new_appt['ActualEndDate'] = data_part1.groupby('AppointmentSerNum').ActualEndDate.apply(set)\n",
    "new_appt['ActualEndDate'] = new_appt['ActualEndDate'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start dxt_AliasName')\n",
    "new_appt['dxt_AliasName'] = data_part1.groupby('AppointmentSerNum').dxt_AliasName.apply(set)\n",
    "new_appt['dxt_AliasName'] = new_appt['dxt_AliasName'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start AliasSerNum')\n",
    "new_appt['AliasSerNum'] = data_part1.groupby('AppointmentSerNum').AliasSerNum.apply(set)\n",
    "new_appt['AliasSerNum'] = new_appt['AliasSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start CourseSerNum')\n",
    "new_appt['CourseSerNum'] = data_part1.groupby('AppointmentSerNum').CourseSerNum.apply(set)\n",
    "new_appt['CourseSerNum'] = new_appt['CourseSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start PlanSerNum')\n",
    "new_appt['PlanSerNum'] = data_part1.groupby('AppointmentSerNum').PlanSerNum.apply(set)\n",
    "new_appt['PlanSerNum'] = new_appt['PlanSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start TreatmentOrientation')\n",
    "new_appt['TreatmentOrientation'] = data_part1.groupby('AppointmentSerNum').TreatmentOrientation.apply(set)\n",
    "new_appt['TreatmentOrientation'] = new_appt['TreatmentOrientation'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start month')\n",
    "new_appt['month'] = data_part1.groupby('AppointmentSerNum').month.apply(set)\n",
    "new_appt['month'] = new_appt['month'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start week')\n",
    "new_appt['week'] = data_part1.groupby('AppointmentSerNum').week.apply(set)\n",
    "new_appt['week'] = new_appt['week'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start hour')\n",
    "new_appt['hour'] = data_part1.groupby('AppointmentSerNum').hour.apply(set)\n",
    "new_appt['hour'] = new_appt['hour'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start AppointmentSerNum')\n",
    "new_appt['AppointmentSerNum'] = new_appt.index.tolist()\n",
    "\n",
    "\n",
    "print('Start numberical features')\n",
    "print('\\nStart age')\n",
    "new_appt['age'] = data_part1.groupby('AppointmentSerNum').age.mean()\n",
    "\n",
    "print('Start Scheduled_duration')\n",
    "new_appt['Scheduled_duration'] = data_part1.groupby('AppointmentSerNum').Scheduled_duration.mean()\n",
    "\n",
    "print('Start Actual_duration')\n",
    "new_appt['Actual_duration'] = data_part1.groupby('AppointmentSerNum').Actual_duration.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_appt_ = new_appt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_part1_grouped = data_part1.groupby('AppointmentSerNum')\n",
    "# count_appt = data_part1_grouped.count()\n",
    "# appt_one_list = count_appt[count_appt.Sex == 1].index.tolist()\n",
    "# appt_more_list = count_appt[count_appt.Sex > 1].index.tolist()\n",
    "\n",
    "# print(f'{len(appt_one_list)} with one appointment')\n",
    "# print(f'{len(appt_more_list)} with more appointment')\n",
    "\n",
    "\n",
    "# # key_list = list(data_part1_grouped.groups.keys())\n",
    "# data_part1_new = pd.DataFrame({})\n",
    "# for i in range(len(appt_more_list)):\n",
    "#     if (i + 1) % 500 == 0:\n",
    "#         print(f'\\n{i+1} appointment data')\n",
    "        \n",
    "#     sample = data_part1_grouped.get_group(appt_more_list[i])\n",
    "#     new_appt = pd.DataFrame({\n",
    "#         'PatientSerNum': sample.PatientSerNum.tolist()[0],\n",
    "#         'AppointmentSerNum': sample.AppointmentSerNum.tolist()[0],\n",
    "#         'Sex': sample.Sex.tolist()[0],\n",
    "#         'age': sample.age.tolist()[0],\n",
    "#         'DoctorSerNum': sample.DoctorSerNum.tolist()[0],\n",
    "#         'month': sample.month.tolist()[0],\n",
    "#         'date': sample.date.tolist()[0],\n",
    "#         'week': sample.week.tolist()[0],\n",
    "#         'hour': sample.hour.tolist()[0],\n",
    "#         'Scheduled_duration': sample.Scheduled_duration.tolist()[0],\n",
    "#         'Actual_duration': sample.Actual_duration.tolist()[0],\n",
    "#         'ScheduledStartTime': sample.ScheduledStartTime.tolist()[0],\n",
    "#         'ScheduledEndTime': sample.ScheduledEndTime.tolist()[0],\n",
    "#         'ActualStartDate': sample.ActualStartDate.tolist()[0],\n",
    "#         'ActualEndDate': sample.ActualEndDate.tolist()[0],\n",
    "        \n",
    "#         'dxt_AliasName': [list(set(sample.dxt_AliasName.tolist()))], \n",
    "#         'AliasSerNum': [list(set(sample.AliasSerNum.tolist()))], \n",
    "#         'CourseSerNum': [list(set(sample.CourseSerNum.tolist()))], \n",
    "#         'PlanSerNum': [list(set(sample.PlanSerNum.tolist()))], \n",
    "#         'TreatmentOrientation': [list(set(sample.TreatmentOrientation.tolist()))],\n",
    "        \n",
    "#     })\n",
    "    \n",
    "#     data_part1_new = pd.concat([data_part1_new, new_appt], axis = 0)\n",
    "# data_part1_new.sort_values(by = ['PatientSerNum', 'AppointmentSerNum', 'ScheduledStartTime'], inplace = True)\n",
    "# data_part1_new.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start categorical features\n",
      "\n",
      "Start FractionNumber\n",
      "Start UserName\n",
      "Start RadiationSerNum\n",
      "Start RadiationId\n",
      "Start ResourceSerNum\n",
      "Start CourseId\n",
      "Start PatientSerNum\n",
      "Start date\n",
      "Start numberical features\n",
      "\n",
      "Start ImagesTaken_total\n",
      "Start MU_total\n",
      "Start MUCoeff_total\n",
      "Start TreatmentTime_total\n"
     ]
    }
   ],
   "source": [
    "new_treat = pd.DataFrame({})\n",
    "\n",
    "print('Start categorical features')\n",
    "print('\\nStart FractionNumber')\n",
    "new_treat['FractionNumber'] = data_part2.groupby(['PatientSerNum', 'date']).PatientSerNum.apply(set)\n",
    "new_treat['FractionNumber'] = new_treat['FractionNumber'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start UserName')\n",
    "new_treat['UserName'] = data_part2.groupby(['PatientSerNum', 'date']).UserName.apply(set)\n",
    "new_treat['UserName'] = new_treat['UserName'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start RadiationSerNum')\n",
    "new_treat['RadiationSerNum'] = data_part2.groupby(['PatientSerNum', 'date']).RadiationSerNum.apply(set)\n",
    "new_treat['RadiationSerNum'] = new_treat['RadiationSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start RadiationId')\n",
    "new_treat['RadiationId'] = data_part2.groupby(['PatientSerNum', 'date']).RadiationId.apply(set)\n",
    "new_treat['RadiationId'] = new_treat['RadiationId'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ResourceSerNum')\n",
    "new_treat['ResourceSerNum'] = data_part2.groupby(['PatientSerNum', 'date']).ResourceSerNum.apply(set)\n",
    "new_treat['ResourceSerNum'] = new_treat['ResourceSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start CourseId')\n",
    "new_treat['CourseId'] = data_part2.groupby(['PatientSerNum', 'date']).CourseId.apply(set)\n",
    "new_treat['CourseId'] = new_treat['CourseId'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start PatientSerNum')\n",
    "new_treat['PatientSerNum'] = new_treat.index.get_level_values(level = 0).tolist()\n",
    "\n",
    "print('Start date')\n",
    "new_treat['date'] = new_treat.index.get_level_values(level = 1).tolist()\n",
    "\n",
    "\n",
    "print('Start numberical features')\n",
    "print('\\nStart ImagesTaken_total')\n",
    "new_treat['ImagesTaken_total'] = data_part2.groupby(['PatientSerNum', 'date']).ImagesTaken.sum()\n",
    "\n",
    "print('Start MU_total')\n",
    "new_treat['MU_total'] = data_part2.groupby(['PatientSerNum', 'date']).MU.sum()\n",
    "\n",
    "print('Start MUCoeff_total')\n",
    "new_treat['MUCoeff_total'] = data_part2.groupby(['PatientSerNum', 'date']).MUCoeff.sum()\n",
    "\n",
    "print('Start TreatmentTime_total')\n",
    "new_treat['TreatmentTime_total'] = data_part2.groupby(['PatientSerNum', 'date']).TreatmentTime.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_treat_ = new_treat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_appt_.reset_index(drop = True, inplace = True)\n",
    "\n",
    "new_treat_.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_part2_grouped = data_part2.groupby(['PatientSerNum', 'date'])\n",
    "# count_pat_appt = data_part2_grouped.count()\n",
    "# treat_one_list = count_pat_appt[count_pat_appt.RadiationHstryAriaSer == 1].index.tolist()\n",
    "# treat_more_list = count_pat_appt[count_pat_appt.RadiationHstryAriaSer > 1].index.tolist()\n",
    "\n",
    "# print(f'{len(treat_one_list)} with one treatment')\n",
    "# print(f'{len(treat_more_list)} with more treatment')\n",
    "\n",
    "\n",
    "# # key_list = list(data_part2_grouped.groups.keys())\n",
    "# data_part2_new = pd.DataFrame({})\n",
    "# for i in range(len(treat_more_list)):\n",
    "#     if (i+1) % 500 == 0:\n",
    "#         print(f'\\n{i+1} treatment data')\n",
    "        \n",
    "#     sample = data_part2_grouped.get_group(treat_more_list[i])\n",
    "#     try:\n",
    "#         new_treat = pd.DataFrame({\n",
    "#             'PatientSerNum': sample.PatientSerNum.tolist()[0],\n",
    "#             'FractionNumber': sample.FractionNumber.tolist()[0],\n",
    "#             'date': sample.date.tolist()[0],\n",
    "\n",
    "#             'UserName': [list(set(sample.UserName.tolist()))], \n",
    "#             'RadiationSerNum': [list(set(sample.RadiationSerNum.tolist()))], \n",
    "#             'RadiationId': [list(set(sample.RadiationId.tolist()))], \n",
    "#             'ResourceSerNum': [list(set(sample.ResourceSerNum.tolist()))], \n",
    "#             'CourseId': [list(set(sample.CourseId.tolist()))],\n",
    "\n",
    "#             'ImagesTaken_total': sum(sample.ImagesTaken.tolist()),\n",
    "#             'MU_total': sum(sample.MU.tolist()),\n",
    "#             'MUCoeff_total': sum(sample.MUCoeff.tolist()),\n",
    "#             'TreatmentTime_total': sum(sample.TreatmentTime.tolist()),\n",
    "#         })\n",
    "#         data_part2_new = pd.concat([data_part2_new, new_treat], axis = 0)\n",
    "#     except:\n",
    "#         print(treat_more_list[i])\n",
    "    \n",
    "    \n",
    "# data_part2_new.sort_values(by = ['PatientSerNum', 'date', 'FractionNumber'], inplace = True)\n",
    "# data_part2_new.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_part1_new.to_csv(pat_duration_LSTM_data_path + 'data_part1_new.csv')\n",
    "# data_part2_new.to_csv(pat_duration_LSTM_data_path + 'data_part2_new.csv')\n",
    "\n",
    "# data_part1_new = pd.read_csv(pat_duration_LSTM_data_path + 'data_part1_new.csv')\n",
    "# data_part2_new = pd.read_csv(pat_duration_LSTM_data_path + 'data_part2_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data_part1_new is (251892, 21)\n",
      "The shape of data_part1_new is (142560, 13)\n",
      "The shape of data is (106232, 32)\n"
     ]
    }
   ],
   "source": [
    "data_part1_new.head()\n",
    "print(f'The shape of data_part1_new is {data_part1_new.shape}')\n",
    "print(f'The shape of data_part1_new is {data_part2_new.shape}')\n",
    "\n",
    "DATA = pd.merge(data_part1_new, data_part2_new, on = ['PatientSerNum', 'date'], how = 'inner')\n",
    "DATA.sort_values(by = ['PatientSerNum', 'AppointmentSerNum', 'ScheduledStartTime', 'FractionNumber'], inplace = True)\n",
    "print(f'The shape of data is {DATA.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series_seq 6236\n",
      "series_one 575\n"
     ]
    }
   ],
   "source": [
    "DATA_grouped = DATA.groupby('PatientSerNum')\n",
    "\n",
    "series_count = DATA_grouped.count()\n",
    "series_seq = series_count[series_count.AppointmentSerNum > 1].index.tolist()\n",
    "series_one = series_count[series_count.AppointmentSerNum == 1].index.tolist()\n",
    "print(f'series_seq {len(series_seq)}')\n",
    "print(f'series_one {len(series_one)}')\n",
    "\n",
    "series_seq = pd.DataFrame({'PatientSerNum': series_seq})\n",
    "series_data = pd.merge(series_seq, DATA, on = 'PatientSerNum', how = 'inner')\n",
    "series_data.sort_values(by = ['PatientSerNum', 'ScheduledStartTime'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of patient is 6236\n",
      "The max length of patient is 80\n",
      "The number of input encoded-feature is 438\n"
     ]
    }
   ],
   "source": [
    "series_data_grouped = series_data.groupby('PatientSerNum')\n",
    "pat_list = list(series_data_grouped.groups.keys())\n",
    "APPT_LEN = series_data_grouped.count().AppointmentSerNum.max()\n",
    "FEATURE_LEN = 438\n",
    "print(f'The total number of patient is {len(pat_list)}')\n",
    "print(f'The max length of patient is {APPT_LEN}')\n",
    "print(f'The number of input encoded-feature is {FEATURE_LEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(train_sample, APPT_LEN, FEATURE_LEN):\n",
    "    \n",
    "    # 样本标签\n",
    "    train_y = np.array([train_sample.Actual_duration.iloc[-1]])\n",
    "\n",
    "    # 最后一个appointment 为我们需要预测的真实治疗时长所对应的appointment，所以需要设置为0\n",
    "    train_sample.Actual_duration.iloc[-1] = 0\n",
    "\n",
    "    # 因为存在相隔很远的两次预约，因此，构造特征Interval_scheduled 来度量两次预约之间的距离\n",
    "    ## 上一次的预期治疗开始时间\n",
    "    train_sample['Last_ScheduledStartTime'] = train_sample.ScheduledStartTime.shift(\n",
    "        periods = 1, fill_value = train_sample.ScheduledStartTime.iloc[0])\n",
    "    ## 需要将时间戳从字符型转为datetime 类型，从而计算时间间隔\n",
    "    train_sample['Last_ScheduledStartTime'] = train_sample.Last_ScheduledStartTime.apply(lambda x: str_to_Datetime(x))\n",
    "    train_sample['ScheduledStartTime'] = train_sample.ScheduledStartTime.apply(lambda x: str_to_Datetime(x))\n",
    "    ## 时间间隔\n",
    "    train_sample['Interval_scheduled'] = train_sample.apply(\n",
    "        lambda x: (x.ScheduledStartTime - x.Last_ScheduledStartTime).days, axis = 1)\n",
    "    \n",
    "    # 对分类变量进行one-hot encoding处理\n",
    "    encode_cate = pd.DataFrame({})\n",
    "    \n",
    "    # 这个地方将Sex 作为序列的一部分，并不是在最后的隐藏层进行合并\n",
    "    encode_cate['Sex'] = train_sample['Sex'].apply(\n",
    "        lambda x: sum(label_encoder_Sex.transform(np.array(x).reshape(-1,1))))\n",
    "\n",
    "    encode_cate['dxt_AliasName'] = train_sample['dxt_AliasName'].apply(\n",
    "        lambda x: sum(label_encoder_dxt_AliasName.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.vstack(encode_cate.dxt_AliasName.tolist())\n",
    "\n",
    "    encode_cate['AliasSerNum'] = train_sample['AliasSerNum'].apply(\n",
    "        lambda x: sum(label_encoder_AliasSerNum.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.AliasSerNum.tolist())))\n",
    "\n",
    "    encode_cate['month'] = train_sample['month'].apply(\n",
    "        lambda x: sum(label_encoder_month.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.month.tolist())))\n",
    "\n",
    "    encode_cate['week'] = train_sample['week'].apply(\n",
    "        lambda x: sum(label_encoder_week.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.week.tolist())))\n",
    "\n",
    "    encode_cate['hour'] = train_sample['hour'].apply(\n",
    "        lambda x: sum(label_encoder_hour.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.hour.tolist())))\n",
    "\n",
    "    encode_cate['DoctorSerNum'] = train_sample['DoctorSerNum'].apply(\n",
    "        lambda x: sum(label_encoder_DoctorSerNum.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.DoctorSerNum.tolist())))\n",
    "\n",
    "    encode_cate['TreatmentOrientation'] = train_sample['TreatmentOrientation'].apply(\n",
    "        lambda x: sum(label_encoder_TreatmentOrientation.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.TreatmentOrientation.tolist())))\n",
    "\n",
    "    encode_cate['FractionNumber'] = train_sample['FractionNumber'].apply(\n",
    "        lambda x: sum(label_encoder_FractionNumber.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.FractionNumber.tolist())))\n",
    "\n",
    "    encode_cate['UserName'] = train_sample['UserName'].apply(\n",
    "        lambda x: sum(label_encoder_UserName.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.UserName.tolist())))\n",
    "\n",
    "    encode_cate['CourseId'] = train_sample['CourseId'].apply(\n",
    "        lambda x: sum(label_encoder_CourseId.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.CourseId.tolist())))\n",
    "\n",
    "    encode_cate['ResourceSerNum'] = train_sample['ResourceSerNum'].apply(\n",
    "        lambda x: sum(label_encoder_ResourceSerNum.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.ResourceSerNum.tolist())))\n",
    "    \n",
    "    # 将数值变量和非数值变量进行合并\n",
    "    train_num = train_sample[feature_num]\n",
    "    train_x = np.hstack((train_x, train_num))\n",
    "    \n",
    "#     # 需要满足序列长度的要求，因此对于短序列进行补零操作\n",
    "#     zeros = np.zeros((APPT_LEN - train_x.shape[0], FEATURE_LEN))\n",
    "#     train_x = np.vstack((zeros, train_x))\n",
    "    \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Aladeeb Ems' in list(label_encoder_UserName.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Aladeeb Ems' in list(data_part2.UserName.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['vcaissie']\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_data.UserName.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['vcaissie']\", \"['amahbub']\", \"['sfournie']\", \"['rbasdeo']\",\n",
       "       \"['cguedes']\", \"['jleclerc']\", \"['kcrawley']\", \"['abelli']\",\n",
       "       \"['swerbrou']\", \"['jlieu']\", \"['jstachura']\", \"['sleichen']\",\n",
       "       \"['aseevara']\", \"['ltasillo']\", \"['sfournie', 'jstachura']\",\n",
       "       \"['rkarant']\", \"['jbattista']\", \"['aseevara', 'rkarant']\",\n",
       "       \"['mcaulfield']\", \"['epagter']\", \"['svilleneuve']\", \"['sgaglia']\",\n",
       "       \"['adicorpo']\", \"['eliu']\", \"['vmarsillo']\", \"['midris']\",\n",
       "       \"['rclark']\", \"['snowac']\", \"['agilcher']\", \"['mkarakova']\",\n",
       "       \"['ijawandha']\", \"['lgoldenberg']\",\n",
       "       \"['Boisvert-Huneault Christian']\", \"['Vachon Suzie']\",\n",
       "       \"['nowac sydnee']\", \"['Aladeeb Emsal']\", \"['poon']\",\n",
       "       \"['jawandha indudeep']\", \"['snelson']\", \"['jrabanal']\",\n",
       "       \"['Unknown', 'jlieu']\", \"['sillick']\", \"['eli']\", \"['hli']\",\n",
       "       \"['amendelsohn']\", \"['Unknown', 'ltasillo']\", \"['rislam']\",\n",
       "       \"['Unknown', 'gdabizha']\", \"['Unknown', 'mpeuckert']\", \"['mlin']\",\n",
       "       \"['Chen Shi Lei']\", \"['Stachura Jacob']\", \"['Lin Michael']\",\n",
       "       \"['Mistry Jayesha']\", \"['mnirula']\", \"['rsanchez']\",\n",
       "       \"['mpeuckert']\", \"['gebrahim']\", \"['nirula manik', 'rruo']\",\n",
       "       \"['Siano']\", \"['Lieu']\", \"['nirula manik']\", \"['jmistry']\",\n",
       "       \"['esirois']\", \"['svachon']\", \"['Li', 'Sirois']\", \"['mlanglais']\",\n",
       "       \"['dufour', 'Chen Shi Lei']\", \"['dufour']\", \"['Li']\",\n",
       "       \"['nowac sydnee', 'Caissie']\", \"['blundell beverly']\",\n",
       "       \"['Unknown', 'vcaissie']\", \"['Seevaratnam']\", \"['Rabanal Javier']\",\n",
       "       \"['Basdeo Reeta']\", \"['gdabizha']\", \"['gebrahim', 'Unknown']\",\n",
       "       \"['hkacemi']\", \"['ypoon']\", \"['de Pagter']\", \"['bblundel']\",\n",
       "       \"['Langlais Marianne']\", \"['Unknown', 'mlanglais']\",\n",
       "       \"['cboisvert']\", \"['Unknown', 'rkarant']\", \"['Unknown', 'ypoon']\",\n",
       "       \"['Unknown', 'esirois']\", \"['wgu']\",\n",
       "       \"['dufour', 'blundell beverly']\", \"['Unknown', 'jbattista']\",\n",
       "       \"['dmart']\", \"['kacemi']\", \"['Mahbub Alfi', 'Caissie']\",\n",
       "       \"['Caissie']\", \"['svachon', 'Unknown']\", \"['Unknown', 'snowac']\",\n",
       "       \"['Sirois']\", \"['cmurphy']\", \"['hsicotte']\", \"['jwong']\",\n",
       "       \"['Unknown', 'rsanchez']\", \"['Unknown', 'eli']\",\n",
       "       \"['Li', 'dufour']\", \"['chudon']\", \"['asotos']\", \"['murphy']\",\n",
       "       \"['ltasillo', 'kcrawley']\", \"['Unknown', 'aseevara']\",\n",
       "       \"['jawandha indudeep', 'Li']\", \"['aseevara', 'ijawandha']\",\n",
       "       \"['Mahbub Alfi']\", \"['amendelsohn', 'Unknown']\", \"['Li', 'Lieu']\",\n",
       "       \"['karant roman']\", \"['rbasdeo', 'abelli']\",\n",
       "       \"['Unknown', 'ijawandha']\", \"['ltelisma']\", \"['dabizha ganna']\",\n",
       "       \"['rislam', 'sfournie']\", \"['murphy', 'kacemi']\",\n",
       "       \"['Unknown', 'epagter']\", \"['Li', 'Mahbub Alfi']\",\n",
       "       \"['Unknown', 'jmistry']\", \"['svachon', 'jlieu']\",\n",
       "       \"['aseevara', 'jmistry']\", \"['esirois', 'rkarant']\",\n",
       "       \"['jlieu', 'rkarant']\", \"['svachon', 'jrabanal']\",\n",
       "       \"['Unknown', 'jstachura']\",\n",
       "       \"['jawandha indudeep', 'Li', 'telisma']\",\n",
       "       \"['jawandha indudeep', 'nelson', 'Gluszko', 'nowac sydnee']\",\n",
       "       \"['nelson']\", \"['esirois', 'aseevara']\", \"['jmylonakis']\",\n",
       "       \"['amendelsohn', 'rbasdeo']\", \"['amendelsohn', 'cmurphy']\",\n",
       "       \"['amendelsohn', 'ijawandha']\", \"['Unknown', 'mnirula']\",\n",
       "       \"['Mahbub Alfi', 'Langlais Marianne']\",\n",
       "       \"['Battista John', 'Rabanal Javier']\", \"['amahbub', 'Unknown']\",\n",
       "       \"['sfournie', 'abelli']\", \"['aseevara', 'jstachura']\",\n",
       "       \"['epagter', 'vcaissie']\", \"['kacemi', 'Rabanal Javier']\",\n",
       "       \"['telisma']\", \"['Unknown', 'hli']\", \"['sleichen', 'jstachura']\",\n",
       "       \"['Unknown', 'cboisvert']\", \"['Unknown', 'sleichen']\",\n",
       "       \"['Li', 'Basdeo Reeta']\", \"['Mendelsohn Amanda']\",\n",
       "       \"['hli', 'jstachura']\", \"['epagter', 'hli']\",\n",
       "       \"['Unknown', 'rbasdeo']\", \"['Stachura Jacob', 'Chen Shi Lei']\",\n",
       "       \"['jgluszko']\", \"['marti']\", \"['hudon']\",\n",
       "       \"['mpeuckert', 'kcrawley']\",\n",
       "       \"['jawandha indudeep', 'nowac sydnee']\", \"['ebrahimi']\",\n",
       "       \"['aseevara', 'ltasillo']\", \"['jlieu', 'sfournie']\",\n",
       "       \"['aseevara', 'mpeuckert']\", \"['esirois', 'sgaglia']\",\n",
       "       \"['sleichen', 'mcaulfield']\", \"['vcaissie', 'hli']\",\n",
       "       \"['Unknown', 'jrabanal']\", \"['Seevaratnam', 'Rabanal Javier']\",\n",
       "       \"['de Pagter', 'Rabanal Javier']\",\n",
       "       \"['Unknown', 'aseevara', 'jrabanal']\", \"['goldenberg l']\",\n",
       "       \"['vcaissie', 'abelli']\", \"['Unknown', 'mkarakova']\",\n",
       "       \"['Aladeeb Emsal', 'dufour']\", \"['karant roman', 'Li']\",\n",
       "       \"['Basdeo Reeta', 'Langlais Marianne']\", \"['Unknown', 'sgaglia']\",\n",
       "       \"['cguedes', 'jmistry']\", \"['mkarakova', 'jmistry']\",\n",
       "       \"['Li', 'Lin Michael']\", \"['mcmanus']\",\n",
       "       \"['Mahbub Alfi', 'ebrahimi']\",\n",
       "       \"['mcmanus', 'Boisvert-Huneault Christian']\",\n",
       "       \"['Boisvert-Huneault Christian', 'Aladeeb Emsal']\",\n",
       "       \"['Siano', 'Aladeeb Emsal']\", \"['Caissie', 'Rabanal Javier']\",\n",
       "       \"['Battista John']\", \"['hsicotte', 'Unknown']\",\n",
       "       \"['vcaissie', 'rkarant']\", \"['telisma', 'karant roman']\",\n",
       "       \"['Siano', 'Caissie']\", \"['cguedes', 'jstachura']\",\n",
       "       \"['aseevara', 'sfournie']\", \"['Sirois', 'Rabanal Javier']\",\n",
       "       \"['nowac sydnee', 'poon']\", \"['de Pagter', 'nirula manik']\",\n",
       "       \"['dufour', 'Aladeeb Emsal']\", \"['karant roman', 'nirula manik']\",\n",
       "       \"['dabizha ganna', 'nowac sydnee']\",\n",
       "       \"['Basdeo Reeta', 'goldenberg l']\", \"['Siano', 'kacemi']\",\n",
       "       \"['karant roman', 'Li', 'dufour']\", \"['Li', 'Caissie']\",\n",
       "       \"['snelson', 'hkacemi']\", \"['sotos']\", \"['Li', 'nowac sydnee']\",\n",
       "       \"['jawandha indudeep', 'Caissie']\", \"['Li', 'Seevaratnam']\",\n",
       "       \"['de Pagter', 'dufour']\", \"['svachon', 'Unknown', 'rkarant']\",\n",
       "       \"['agilcher', 'midris']\", \"['snowac', 'gdabizha']\",\n",
       "       \"['poon', 'blundell beverly']\", \"['Seevaratnam', 'Caissie']\",\n",
       "       \"['jawandha indudeep', 'blundell beverly', 'dufour']\",\n",
       "       \"['Langlais Marianne', 'Rabanal Javier']\",\n",
       "       \"['Li', 'Aladeeb Emsal']\", \"['jawandha indudeep', 'dufour']\",\n",
       "       \"['mlanglais', 'hli']\", \"['Vachon Suzie', 'dabizha ganna']\",\n",
       "       \"['nirula manik', 'Sirois']\", \"['rislam', 'Unknown']\",\n",
       "       \"['poon', 'dufour']\", \"['Li', 'blundell beverly', 'dufour']\",\n",
       "       \"['karant roman', 'Li', 'telisma']\", \"['Li', 'poon']\",\n",
       "       \"['dufour', 'karant roman', 'poon']\",\n",
       "       \"['karant roman', 'Aladeeb Emsal']\",\n",
       "       \"['Langlais Marianne', 'kacemi']\", \"['Raffis Nancy']\",\n",
       "       \"['rossel']\", \"['Raffis Nancy', 'Rabanal Javier']\",\n",
       "       \"['mcmanus', 'ebrahimi']\", \"['Lieu', 'poon']\",\n",
       "       \"['Sirois', 'Seevaratnam']\", \"['Caissie', 'kacemi']\",\n",
       "       \"['Basdeo Reeta', 'dufour']\", \"['Unknown', 'rkarant', 'rbasdeo']\",\n",
       "       \"['Li', 'mcmanus']\", \"['Unknown', 'ltelisma']\",\n",
       "       \"['Sirois', 'Basdeo Reeta']\", \"['Li', 'Chen Shi Lei']\",\n",
       "       \"['hsicotte', 'esirois']\", \"['srossel']\",\n",
       "       \"['Raffis Nancy', 'Aladeeb Emsal']\",\n",
       "       \"['Battista John', 'Aladeeb Emsal']\",\n",
       "       \"['Battista John', 'Aladeeb Emsal', 'dufour']\",\n",
       "       \"['jawandha indudeep', 'Aladeeb Emsal']\",\n",
       "       \"['Seevaratnam', 'blundell beverly']\",\n",
       "       \"['Boisvert-Huneault Christian', 'Langlais Marianne']\",\n",
       "       \"['nirula manik', 'karant roman']\", \"['Stachura Jacob', 'Li']\",\n",
       "       \"['Gluszko', 'poon', 'kacemi']\", \"['Basdeo Reeta', 'poon']\",\n",
       "       \"['Sirois', 'Vachon Suzie']\", \"['Lieu', 'Sirois']\",\n",
       "       \"['nirula manik', 'Vachon Suzie']\", \"['Sanchez']\",\n",
       "       \"['Li', 'poon', 'dufour']\", \"['Unknown', 'sfournie']\",\n",
       "       \"['Aladeeb Emsal', 'blundell beverly']\",\n",
       "       \"['telisma', 'Rabanal Javier']\", \"['Unknown', 'hkacemi']\",\n",
       "       \"['Li', 'Lin Michael', 'karant roman']\", \"['de Pagter', 'Li']\",\n",
       "       \"['de Pagter', 'blundell beverly']\", \"['de Pagter', 'poon']\",\n",
       "       \"['ijawandha', 'chudon']\",\n",
       "       \"['Boisvert-Huneault Christian', 'Basdeo Reeta']\",\n",
       "       \"['Boisvert-Huneault Christian', 'Vachon Suzie']\",\n",
       "       \"['Basdeo Reeta', 'blundell beverly']\",\n",
       "       \"['telisma', 'nowac sydnee']\", \"['esirois', 'rsanchez']\",\n",
       "       \"['esirois', 'jstachura']\", \"['amahbub', 'esirois']\",\n",
       "       \"['epagter', 'esirois']\",\n",
       "       \"['karant roman', 'Boisvert-Huneault Christian']\",\n",
       "       \"['Lieu', 'Vachon Suzie']\", \"['jawandha indudeep', 'poon']\",\n",
       "       \"['jawandha indudeep', 'dabizha ganna']\", \"['peuckert mariko']\",\n",
       "       \"['Lin Michael', 'Caissie']\", \"['nelson', 'blundell beverly']\",\n",
       "       \"['Mistry Jayesha', 'Caissie']\", \"['nelson', 'Basdeo Reeta']\",\n",
       "       \"['jawandha indudeep', 'Battista John']\",\n",
       "       \"['jawandha indudeep', 'nirula manik', 'Boisvert-Huneault Christian']\",\n",
       "       \"['ypoon', 'mpeuckert']\", \"['de Pagter', 'kacemi']\",\n",
       "       \"['nirula manik', 'jawandha indudeep']\", \"['amahbub', 'snelson']\",\n",
       "       \"['dabizha ganna', 'poon']\", \"['svachon', 'Unknown', 'cboisvert']\",\n",
       "       \"['nirula manik', 'kacemi']\",\n",
       "       \"['jawandha indudeep', 'Seevaratnam', 'Chen Shi Lei']\",\n",
       "       \"['jawandha indudeep', 'nelson']\", \"['jgluszko', 'rkarant']\",\n",
       "       \"['rkarant', 'mkarakova']\", \"['Sirois', 'dabizha ganna']\",\n",
       "       \"['Basdeo Reeta', 'Vachon Suzie']\", \"['Vachon Suzie', 'poon']\",\n",
       "       \"['svachon', 'epagter']\", \"['hsicotte', 'epagter']\",\n",
       "       \"['svachon', 'rkarant']\", \"['sleichen', 'jbattista']\",\n",
       "       \"['hli', 'jbattista']\", \"['hli', 'ijawandha']\",\n",
       "       \"['vcaissie', 'ypoon']\", \"['mpeuckert', 'ijawandha']\",\n",
       "       \"['amendelsohn', 'mlanglais']\", \"['amendelsohn', 'snowac']\",\n",
       "       \"['amendelsohn', 'hli']\", \"['epagter', 'rbasdeo']\",\n",
       "       \"['rkarant', 'mpeuckert']\", \"['mnirula', 'jrabanal']\",\n",
       "       \"['amendelsohn', 'mnirula']\", \"['amendelsohn', 'jrabanal']\",\n",
       "       \"['hkacemi', 'eli']\", \"['Unknown', 'snelson']\",\n",
       "       \"['gebrahim', 'jstachura']\", \"['mlin', 'jstachura']\",\n",
       "       \"['vcaissie', 'jstachura']\", \"['Unknown', 'rclark']\",\n",
       "       \"['snelson', 'mkarakova']\", \"['mkarakova', 'ijawandha']\",\n",
       "       \"['snelson', 'eli']\", \"['gebrahim', 'snelson']\",\n",
       "       \"['mlin', 'mkarakova']\", \"['mlin', 'jbattista']\",\n",
       "       \"['hkacemi', 'rsanchez']\", \"['Unknown', 'mcaulfield']\",\n",
       "       \"['rbasdeo', 'rsanchez']\", \"['rkarant', 'jstachura']\",\n",
       "       \"['mpeuckert', 'jrabanal']\", \"['rbasdeo', 'rkarant']\",\n",
       "       \"['aseevara', 'mkarakova']\", \"['mpeuckert', 'galdosary']\",\n",
       "       \"['vcaissie', 'aseevara']\", \"['Unknown', 'mlin']\",\n",
       "       \"['jbattista', 'ijawandha']\", \"['epagter', 'eli']\",\n",
       "       \"['mlin', 'ijawandha']\", \"['mkarakova', 'jbattista']\",\n",
       "       \"['Unknown', 'lgoldenberg']\", \"['cboisvert', 'abelli']\",\n",
       "       \"['aseevara', 'hkacemi']\", \"['esirois', 'eli']\",\n",
       "       \"['ijawandha', 'jstachura']\", \"['vcaissie', 'jbattista']\",\n",
       "       \"['rkarant', 'snowac']\", \"['mkarakova', 'hkacemi']\",\n",
       "       \"['aseevara', 'mlanglais', 'jstachura']\", \"['rkarant', 'hli']\",\n",
       "       \"['mnirula', 'hli']\", \"['svachon', 'mkarakova']\",\n",
       "       \"['snelson', 'mnirula']\", \"['mkarakova', 'sfournie']\",\n",
       "       \"['svachon', 'snelson']\", \"['mkarakova', 'jstachura']\",\n",
       "       \"['sleichen', 'mnirula']\", \"['hkacemi', 'hli']\",\n",
       "       \"['mkarakova', 'hli']\", \"['mhobson', 'ijawandha']\",\n",
       "       \"['mnirula', 'sfournie']\", \"['mnirula', 'abelli']\",\n",
       "       \"['mnirula', 'hpatroci']\", \"['esirois', 'vcaissie', 'epagter']\",\n",
       "       \"['rbasdeo', 'jrabanal']\", \"['vcaissie', 'snelson']\",\n",
       "       \"['sfournie', 'jbattista']\", \"['jlieu', 'snowac']\",\n",
       "       \"['jlieu', 'rbasdeo']\", \"['mnirula', 'mkarakova']\",\n",
       "       \"['jlieu', 'jrabanal']\", \"['sgaglia', 'rsanchez']\",\n",
       "       \"['Unknown', 'abelli']\", \"['vcaissie', 'hkacemi']\",\n",
       "       \"['svachon', 'sfournie']\", \"['epagter', 'rclark']\",\n",
       "       \"['rsanchez', 'ijawandha']\", \"['gebrahim', 'mkarakova']\",\n",
       "       \"['rclark', 'ijawandha']\", \"['ltasillo', 'snowac']\",\n",
       "       \"['ltasillo', 'aseevara']\", \"['sleichen', 'ltasillo']\",\n",
       "       \"['ltasillo', 'rbasdeo']\", \"['aseevara', 'eli']\",\n",
       "       \"['Unknown', 'eliu']\", \"['amahbub', 'aseevara']\",\n",
       "       \"['jstachura', 'eli']\", \"['vcaissie', 'sfournie']\",\n",
       "       \"['esirois', 'mpeuckert']\", \"['jlieu', 'abelli']\",\n",
       "       \"['hkacemi', 'jrabanal']\", \"['hkacemi', 'rclark']\",\n",
       "       \"['gebrahim', 'aseevara']\", \"['rkarant', 'hkacemi']\",\n",
       "       \"['hkacemi', 'jstachura']\", \"['Unknown', 'adicorpo']\",\n",
       "       \"['jlieu', 'mnirula']\", \"['epagter', 'jstachura']\",\n",
       "       \"['mpeuckert', 'jmistry']\", \"['sgaglia', 'jmistry']\",\n",
       "       \"['epagter', 'sfournie']\", \"['hli', 'chudon']\",\n",
       "       \"['cguedes', 'Unknown']\", \"['amahbub', 'eli']\",\n",
       "       \"['aseevara', 'jrabanal']\", \"['snelson', 'jbattista']\",\n",
       "       \"['ltasillo', 'abelli']\", \"['esirois', 'ijawandha']\",\n",
       "       \"['aseevara', 'hli']\", \"['gebrahim', 'hli']\",\n",
       "       \"['snelson', 'ijawandha']\", \"['esirois', 'hkacemi']\",\n",
       "       \"['amahbub', 'rclark']\", \"['epagter', 'aseevara']\",\n",
       "       \"['rkarant', 'jrabanal']\", \"['epagter', 'jlieu']\",\n",
       "       \"['cguedes', 'sfournie']\", \"['mpeuckert', 'sfournie']\",\n",
       "       \"['Unknown', 'agilcher']\", \"['gebrahim', 'sleichen', 'sgaglia']\",\n",
       "       \"['sgaglia', 'rclark']\", \"['epagter', 'ltasillo']\",\n",
       "       \"['eliu', 'jrabanal']\", \"['snowac', 'jrabanal']\",\n",
       "       \"['rkarant', 'adicorpo']\", \"['eliu', 'sfournie']\",\n",
       "       \"['rislam', 'abelli']\", \"['amendelsohn', 'rclark']\",\n",
       "       \"['rislam', 'sgaglia']\", \"['cguedes', 'adicorpo']\",\n",
       "       \"['epagter', 'rsanchez']\", \"['epagter', 'adicorpo']\",\n",
       "       \"['adicorpo', 'rsanchez']\", \"['abelli', 'jrabanal']\",\n",
       "       \"['rislam', 'ltasillo']\", \"['amahbub', 'jlieu']\",\n",
       "       \"['sgaglia', 'abelli']\", \"['snowac', 'rsanchez']\",\n",
       "       \"['adicorpo', 'eli']\", \"['epagter', 'rkarant']\",\n",
       "       \"['svachon', 'sgaglia']\", \"['vcaissie', 'eli']\",\n",
       "       \"['sfournie', 'jmistry']\", \"['svachon', 'rsanchez']\",\n",
       "       \"['sillick', 'rclark']\", \"['sgaglia', 'snowac']\",\n",
       "       \"['rislam', 'amendelsohn']\", \"['jbattista', 'abelli']\",\n",
       "       \"['Unknown', 'midris']\", \"['vcaissie', 'sgaglia']\",\n",
       "       \"['epagter', 'sillick']\", \"['eliu', 'rclark']\",\n",
       "       \"['adicorpo', 'eliu']\", \"['rclark', 'gdabizha']\",\n",
       "       \"['esirois', 'rclark']\", \"['ltasillo', 'eliu']\",\n",
       "       \"['epagter', 'jbattista']\", \"['agilcher', 'jbattista']\",\n",
       "       \"['amendelsohn', 'abelli']\", \"['vmarsillo', 'rkarant']\",\n",
       "       \"['cguedes', 'vcaissie']\", \"['esirois', 'rbasdeo']\",\n",
       "       \"['vmarsillo', 'eli']\", \"['esirois', 'sleichen']\",\n",
       "       \"['snowac', 'eliu']\", \"['ltasillo', 'midris']\",\n",
       "       \"['aseevara', 'kcrawley']\", \"['sgaglia', 'eli']\",\n",
       "       \"['eliu', 'kcrawley']\", \"['Unknown', 'sillick']\",\n",
       "       \"['amahbub', 'agilcher']\", \"['sgaglia', 'kcrawley']\",\n",
       "       \"['agilcher', 'snowac']\", \"['Unknown', 'kcrawley']\",\n",
       "       \"['Unknown', 'vmarsillo']\", \"['hkacemi', 'jbattista']\",\n",
       "       \"['sgaglia', 'vmarsillo']\", \"['ltasillo', 'vmarsillo']\",\n",
       "       \"['esirois', 'kcrawley']\", \"['rkarant', 'jwong']\",\n",
       "       \"['sillick', 'jwong']\", \"['sillick', 'mpeuckert']\",\n",
       "       \"['agilcher', 'sillick']\", \"['hkacemi', 'gdabizha']\",\n",
       "       \"['rbasdeo', 'hkacemi', 'eli']\", \"['agilcher', 'mpeuckert']\",\n",
       "       \"['ltasillo', 'hkacemi']\", \"['amahbub', 'rislam']\",\n",
       "       \"['vmarsillo', 'ltasillo']\", \"['esirois', 'ltasillo']\",\n",
       "       \"['Unknown', 'jleclerc']\", \"['eliu', 'midris', 'eli']\",\n",
       "       \"['eli', 'kcrawley']\", \"['rclark', 'kcrawley']\",\n",
       "       \"['adicorpo', 'rclark']\", \"['mlanglais', 'rclark']\",\n",
       "       \"['bblundel', 'jbattista']\", \"['rclark', 'midris']\",\n",
       "       \"['rbasdeo', 'rclark']\", \"['kcrawley', 'eli']\", \"['eliu', 'eli']\",\n",
       "       \"['rkarant', 'jbattista']\", \"['vmarsillo', 'sgaglia']\",\n",
       "       \"['agilcher', 'jlieu']\", \"['ltasillo', 'rclark']\",\n",
       "       \"['agilcher', 'eliu', 'rclark']\", \"['rislam', 'eli']\",\n",
       "       \"['svachon', 'ltasillo']\", \"['rkarant', 'dmart']\",\n",
       "       \"['rkarant', 'eliu']\", \"['bblundel', 'cguedes']\",\n",
       "       \"['agilcher', 'eli']\", \"['sgaglia', 'cboisvert']\"], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_data.UserName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, None, 128)         290304    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 310,945\n",
      "Trainable params: 310,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 不指定时间戳长度\n",
    "model_sequence = Sequential()\n",
    "model_sequence.add(layers.LSTM(\n",
    "        batch_input_shape = (None , None, FEATURE_LEN),\n",
    "        output_dim = 128,\n",
    "        dropout=0.1,\n",
    "        recurrent_dropout=0.5,\n",
    "        return_sequences=True,\n",
    "        ))\n",
    "\n",
    "model_sequence.add(layers.LSTM(\n",
    "        output_dim = 32,\n",
    "        dropout=0.1,\n",
    "        recurrent_dropout=0.5,\n",
    "        ))\n",
    "# stateful = True 本次batch的参数返回到下一次的训练中\n",
    "\n",
    "model_sequence.add(layers.Dense(1))\n",
    "\n",
    "model_sequence.compile(\n",
    "        optimizer = 'rmsprop',\n",
    "        loss = 'mae'\n",
    "        )\n",
    "\n",
    "model_sequence.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_train = 40\n",
    "BATCH_test = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: ['Aladeeb Emsa']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cfbddf931281>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpat_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtrain_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_data_grouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtrain_x_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAPPT_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFEATURE_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-f9c58f578139>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[1;34m(train_sample, APPT_LEN, FEATURE_LEN)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     encode_cate['UserName'] = train_sample['UserName'].apply(\n\u001b[1;32m---> 61\u001b[1;33m         lambda x: sum(label_encoder_UserName.transform(np.array(x).reshape(-1,1))))\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode_cate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUserName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-f9c58f578139>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     encode_cate['UserName'] = train_sample['UserName'].apply(\n\u001b[1;32m---> 61\u001b[1;33m         lambda x: sum(label_encoder_UserName.transform(np.array(x).reshape(-1,1))))\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode_cate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUserName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    729\u001b[0m                                        copy=True)\n\u001b[0;32m    730\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[1;34m\"\"\"New implementation assuming categorical input\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[1;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m         \u001b[0mX_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X, handle_unknown)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                     \u001b[0mXi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[0mX_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36m_encode\u001b[1;34m(values, uniques, encode)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_encode_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[1;34m(values, uniques, encode)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             raise ValueError(\"y contains previously unseen labels: %s\"\n\u001b[1;32m---> 49\u001b[1;33m                              % str(diff))\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: ['Aladeeb Emsa']"
     ]
    }
   ],
   "source": [
    "for i in range(int(len(pat_list)/(BATCH_train + BATCH_test))):\n",
    "    print(f'Batch {i}')\n",
    "    # 构造训练数据\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for tr in range((BATCH_train+BATCH_test)*i,\n",
    "                    (BATCH_train+BATCH_test)*(i+1) - BATCH_test):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        train_x_i, train_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        train_x.append(train_x_i)\n",
    "        train_y.append(train_y_i)\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    train_result = model_sequence.train_on_batch(train_x, train_y)\n",
    "        \n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    for te in range((BATCH_train+BATCH_test)*(i+1) - BATCH_test,\n",
    "                    (BATCH_train+BATCH_test)*(i+1)):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        val_x_i, val_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        val_x.append(val_x_i)\n",
    "        val_y.append(val_y_i)\n",
    "    val_x = np.array(val_x)\n",
    "    val_y = np.array(val_y)\n",
    "    break\n",
    "    # model_sequence.train_on_batch(val_x, val_y)\n",
    "    val_result = model_sequence.evaluate(val_x, val_y, verbose=0) # verbose 显示的时候有进度条\n",
    "    print(f'{tr+1 - BATCH_train}-{tr+1}-train [{train_result}] ||| {te+1 - BATCH_test}-{te+1}-validation [{val_result}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_5 (GRU)                  (None, 80, 128)           217728    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 32)                15456     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 233,217\n",
      "Trainable params: 233,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# dropout=0.1,\n",
    "# recurrent_dropout=0.5,\n",
    "\n",
    "model_sequence1 = Sequential()\n",
    "model_sequence1.add(layers.GRU(\n",
    "        batch_input_shape = (None , APPT_LEN, FEATURE_LEN),\n",
    "        output_dim = 128,\n",
    "\n",
    "        return_sequences=True,\n",
    "        ))\n",
    "\n",
    "# dropout=0.1,\n",
    "# recurrent_dropout=0.5,\n",
    "\n",
    "model_sequence1.add(layers.GRU(\n",
    "        output_dim = 32,\n",
    "\n",
    "        ))\n",
    "# stateful = True 本次batch的参数返回到下一次的训练中\n",
    "\n",
    "model_sequence1.add(layers.Dense(1))\n",
    "\n",
    "model_sequence1.compile(\n",
    "        optimizer = 'rmsprop',\n",
    "        loss = 'mae'\n",
    "        )\n",
    "\n",
    "model_sequence1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "0-40-train [29.033550262451172] ||| 40-45-validation [17.26938247680664]\n",
      "Batch 1\n",
      "45-85-train [18.42552375793457] ||| 85-90-validation [15.728889465332031]\n",
      "Batch 2\n",
      "90-130-train [17.172286987304688] ||| 130-135-validation [10.300146102905273]\n",
      "Batch 3\n",
      "135-175-train [20.477676391601562] ||| 175-180-validation [20.931716918945312]\n",
      "Batch 4\n",
      "180-220-train [15.885931015014648] ||| 220-225-validation [14.639741897583008]\n",
      "Batch 5\n",
      "225-265-train [19.633190155029297] ||| 265-270-validation [7.4754228591918945]\n",
      "Batch 6\n",
      "270-310-train [13.881879806518555] ||| 310-315-validation [20.360618591308594]\n",
      "Batch 7\n",
      "315-355-train [14.46210765838623] ||| 355-360-validation [3.0205907821655273]\n",
      "Batch 8\n",
      "360-400-train [12.831311225891113] ||| 400-405-validation [14.861434936523438]\n",
      "Batch 9\n",
      "405-445-train [14.254976272583008] ||| 445-450-validation [5.8323445320129395]\n",
      "Batch 10\n",
      "450-490-train [12.768884658813477] ||| 490-495-validation [6.7624006271362305]\n",
      "Batch 11\n",
      "495-535-train [12.343925476074219] ||| 535-540-validation [16.65709686279297]\n",
      "Batch 12\n",
      "540-580-train [13.27532958984375] ||| 580-585-validation [3.600989580154419]\n",
      "Batch 13\n",
      "585-625-train [13.190481185913086] ||| 625-630-validation [14.562875747680664]\n",
      "Batch 14\n",
      "630-670-train [15.462725639343262] ||| 670-675-validation [5.492772102355957]\n",
      "Batch 15\n",
      "675-715-train [10.875143051147461] ||| 715-720-validation [7.449995517730713]\n",
      "Batch 16\n",
      "720-760-train [13.16185474395752] ||| 760-765-validation [5.439629077911377]\n",
      "Batch 17\n",
      "765-805-train [9.466924667358398] ||| 805-810-validation [9.367010116577148]\n",
      "Batch 18\n",
      "810-850-train [11.547209739685059] ||| 850-855-validation [33.32210159301758]\n",
      "Batch 19\n",
      "855-895-train [12.11719036102295] ||| 895-900-validation [11.279465675354004]\n",
      "Batch 20\n",
      "900-940-train [16.106698989868164] ||| 940-945-validation [3.7650279998779297]\n",
      "Batch 21\n",
      "945-985-train [9.322242736816406] ||| 985-990-validation [43.19862747192383]\n",
      "Batch 22\n",
      "990-1030-train [8.37379264831543] ||| 1030-1035-validation [3.158372402191162]\n",
      "Batch 23\n",
      "1035-1075-train [10.975160598754883] ||| 1075-1080-validation [3.114107608795166]\n",
      "Batch 24\n",
      "1080-1120-train [9.563238143920898] ||| 1120-1125-validation [6.077256679534912]\n",
      "Batch 25\n",
      "1125-1165-train [9.887956619262695] ||| 1165-1170-validation [14.038228988647461]\n",
      "Batch 26\n",
      "1170-1210-train [10.549654006958008] ||| 1210-1215-validation [8.002077102661133]\n",
      "Batch 27\n",
      "1215-1255-train [12.93274211883545] ||| 1255-1260-validation [4.969959259033203]\n",
      "Batch 28\n",
      "1260-1300-train [10.397086143493652] ||| 1300-1305-validation [10.935868263244629]\n",
      "Batch 29\n",
      "1305-1345-train [18.499923706054688] ||| 1345-1350-validation [15.90489673614502]\n",
      "Batch 30\n",
      "1350-1390-train [7.554978847503662] ||| 1390-1395-validation [7.912192344665527]\n",
      "Batch 31\n",
      "1395-1435-train [8.987907409667969] ||| 1435-1440-validation [3.1541738510131836]\n",
      "Batch 32\n",
      "1440-1480-train [9.405927658081055] ||| 1480-1485-validation [3.812253952026367]\n",
      "Batch 33\n",
      "1485-1525-train [11.791290283203125] ||| 1525-1530-validation [4.779034614562988]\n",
      "Batch 34\n",
      "1530-1570-train [8.295125961303711] ||| 1570-1575-validation [3.750781297683716]\n",
      "Batch 35\n",
      "1575-1615-train [9.425666809082031] ||| 1615-1620-validation [1.7165288925170898]\n",
      "Batch 36\n",
      "1620-1660-train [10.703584671020508] ||| 1660-1665-validation [10.684527397155762]\n",
      "Batch 37\n",
      "1665-1705-train [8.683987617492676] ||| 1705-1710-validation [1.1592350006103516]\n",
      "Batch 38\n",
      "1710-1750-train [9.088254928588867] ||| 1750-1755-validation [1.3807296752929688]\n",
      "Batch 39\n",
      "1755-1795-train [8.167673110961914] ||| 1795-1800-validation [0.41464900970458984]\n",
      "Batch 40\n",
      "1800-1840-train [9.455282211303711] ||| 1840-1845-validation [14.549215316772461]\n",
      "Batch 41\n",
      "1845-1885-train [8.640848159790039] ||| 1885-1890-validation [3.517909288406372]\n",
      "Batch 42\n",
      "1890-1930-train [8.108800888061523] ||| 1930-1935-validation [12.482658386230469]\n",
      "Batch 43\n",
      "1935-1975-train [9.83915901184082] ||| 1975-1980-validation [0.45415210723876953]\n",
      "Batch 44\n",
      "1980-2020-train [11.786384582519531] ||| 2020-2025-validation [8.418599128723145]\n",
      "Batch 45\n",
      "2025-2065-train [7.060329437255859] ||| 2065-2070-validation [0.3853330612182617]\n",
      "Batch 46\n",
      "2070-2110-train [8.810149192810059] ||| 2110-2115-validation [4.351316452026367]\n",
      "Batch 47\n",
      "2115-2155-train [9.50554370880127] ||| 2155-2160-validation [0.6827058792114258]\n",
      "Batch 48\n",
      "2160-2200-train [9.206945419311523] ||| 2200-2205-validation [9.284711837768555]\n",
      "Batch 49\n",
      "2205-2245-train [8.41862964630127] ||| 2245-2250-validation [10.249488830566406]\n",
      "Batch 50\n",
      "2250-2290-train [12.751091003417969] ||| 2290-2295-validation [38.21573257446289]\n",
      "Batch 51\n",
      "2295-2335-train [8.283462524414062] ||| 2335-2340-validation [2.177316665649414]\n",
      "Batch 52\n",
      "2340-2380-train [14.52015209197998] ||| 2380-2385-validation [0.7885618209838867]\n",
      "Batch 53\n",
      "2385-2425-train [10.61923885345459] ||| 2425-2430-validation [1.1009511947631836]\n",
      "Batch 54\n",
      "2430-2470-train [9.967939376831055] ||| 2470-2475-validation [35.06837463378906]\n",
      "Batch 55\n",
      "2475-2515-train [9.154203414916992] ||| 2515-2520-validation [3.0340394973754883]\n",
      "Batch 56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c7e44abeab68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpat_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtrain_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_data_grouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtrain_x_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAPPT_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFEATURE_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-930521d87647>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[1;34m(train_sample, APPT_LEN, FEATURE_LEN)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m## 时间间隔\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     train_sample['Interval_scheduled'] = train_sample.apply(\n\u001b[1;32m---> 18\u001b[1;33m         lambda x: (x.ScheduledStartTime - x.Last_ScheduledStartTime).days, axis = 1)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# 对分类变量进行one-hot encoding处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3369\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3370\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3450\u001b[0m         \u001b[1;31m# value exception to occur first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3452\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[1;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[0;32m   3244\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3245\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3246\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3247\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3248\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "for i in range(int(len(pat_list)/(BATCH_train + BATCH_test))):\n",
    "    print(f'Batch {i}')\n",
    "    # 构造训练数据\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for tr in range((BATCH_train+BATCH_test)*i,\n",
    "                    (BATCH_train+BATCH_test)*(i+1) - BATCH_test):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        train_x_i, train_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        train_x.append(train_x_i)\n",
    "        train_y.append(train_y_i)\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    train_result = model_sequence1.train_on_batch(train_x, train_y)\n",
    "        \n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    for te in range((BATCH_train+BATCH_test)*(i+1) - BATCH_test,\n",
    "                    (BATCH_train+BATCH_test)*(i+1)):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        val_x_i, val_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        val_x.append(val_x_i)\n",
    "        val_y.append(val_y_i)\n",
    "    val_x = np.array(val_x)\n",
    "    val_y = np.array(val_y)\n",
    "    # model_sequence.train_on_batch(val_x, val_y)\n",
    "    val_result = model_sequence1.evaluate(val_x, val_y, verbose=0) # verbose 显示的时候有进度条\n",
    "    print(f'{tr+1 - BATCH_train}-{tr+1}-train [{train_result}] ||| {te+1 - BATCH_test}-{te+1}-validation [{val_result}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
