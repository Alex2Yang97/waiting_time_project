{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(r'D:\\jupyter files\\waiting_time_project\\my_tools')\n",
    "import tools_for_os.for_df as ml_df\n",
    "import tools_for_os.for_file as ml_fl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\jupyter files\\data_waiting_time_project\\preprocess_data\\patient_duration_LSTM_data\\ already existed!\n"
     ]
    }
   ],
   "source": [
    "data_path = 'D:\\\\jupyter files\\\\data_waiting_time_project\\\\preprocess_data\\\\'\n",
    "\n",
    "pat_duration_LSTM_data_path = data_path + 'patient_duration_LSTM_data\\\\'\n",
    "ml_fl.create_folder(pat_duration_LSTM_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_part1 = pd.read_csv(data_path + 'data_part1.csv', index_col = 0)\n",
    "data_part2 = pd.read_csv(data_path + 'data_part2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把字符串转成datetime\n",
    "def str_to_Datetime(st):\n",
    "    dt = datetime.datetime.strptime(st, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num = ['Scheduled_duration', 'Actual_duration',\n",
    "               'age', 'TreatmentTime_total', 'ImagesTaken_total',\n",
    "               'MU_total', 'MUCoeff_total', 'Interval_scheduled']\n",
    "\n",
    "# RadiationId\n",
    "feature_cate = ['dxt_AliasName', 'Sex', 'AliasSerNum',\n",
    "                'month', 'week', 'hour', 'DoctorSerNum', \n",
    "                'TreatmentOrientation', 'FractionNumber',\n",
    "                'UserName', 'CourseId', 'ResourceSerNum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为需要进行onehot encoding，所以在拼接数据之前，先进行数据格式的处理\n",
    "for col in feature_cate:\n",
    "    try:\n",
    "        data_part1[col].fillna('Unknown', inplace = True)\n",
    "        data_part1[col] = data_part1[col].astype(str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        data_part2[col].fillna('Unknown', inplace = True)\n",
    "        data_part2[col] = data_part2[col].astype(str)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for col in feature_num:\n",
    "    try:\n",
    "        data_part1.fillna(0, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        data_part2.fillna(0, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# data_num = log1p(data_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "              n_values=None, sparse=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoder\n",
    "label_encoder_dxt_AliasName = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_dxt_AliasName.fit(data_part1.dxt_AliasName.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_Sex = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_Sex.fit(data_part1.Sex.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_AliasSerNum = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_AliasSerNum.fit(data_part1.AliasSerNum.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_month = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_month.fit(data_part1.month.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_week = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_week.fit(data_part1.week.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_hour = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_hour.fit(data_part1.hour.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_DoctorSerNum = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_DoctorSerNum.fit(data_part1.DoctorSerNum.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_TreatmentOrientation = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_TreatmentOrientation.fit(data_part1.TreatmentOrientation.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_FractionNumber = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_FractionNumber.fit(data_part2.FractionNumber.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_UserName = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_UserName.fit(data_part2.UserName.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_CourseId = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_CourseId.fit(data_part2.CourseId.values.reshape(-1, 1))\n",
    "\n",
    "label_encoder_ResourceSerNum = preprocessing.OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "label_encoder_ResourceSerNum.fit(data_part2.ResourceSerNum.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorical feature 不同取值的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_num = ['Scheduled_duration', 'Actual_duration', 'Actual_duration',\n",
    "#                'age', 'TreatmentTime_total', 'ImagesTaken_total',\n",
    "#                'MU_total', 'MUCoeff_total']\n",
    "\n",
    "# # RadiationId\n",
    "# feature_cate = ['dxt_AliasName', 'Sex', 'AliasSerNum',\n",
    "#                 'month', 'week', 'hour', 'DoctorSerNum', \n",
    "#                 'TreatmentOrientation', 'FractionNumber',\n",
    "#                 'UserName', 'CourseId', 'ResourceSerNum']\n",
    "\n",
    "# feature_count1 = pd.DataFrame({})\n",
    "# for col in feature_cate:\n",
    "#     try:\n",
    "#         n = len(data_part1[col].unique())\n",
    "#         feature_count1[col] = [n]\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "# feature_count2 = pd.DataFrame({})\n",
    "# for col in feature_cate:\n",
    "#     try:\n",
    "#         n = len(data_part2[col].unique())\n",
    "#         feature_count2[col] = [n]\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69142 with one appointment\n",
      "251892 with more appointment\n"
     ]
    }
   ],
   "source": [
    "data_part1_grouped = data_part1.groupby('AppointmentSerNum')\n",
    "count_appt = data_part1_grouped.count()\n",
    "appt_one_list = count_appt[count_appt.Sex == 1].index.tolist()\n",
    "appt_more_list = count_appt[count_appt.Sex > 1].index.tolist()\n",
    "\n",
    "print(f'{len(appt_one_list)} with one appointment')\n",
    "print(f'{len(appt_more_list)} with more appointment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(x):\n",
    "    x = list(x)\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cateorical features\n",
      "\n",
      "Start PatientSerNum\n",
      "Start Sex\n",
      "Start DoctorSerNum\n",
      "Start date\n",
      "Start ScheduledStartTime\n",
      "Start ScheduledEndTime\n",
      "Start ActualStartDate\n",
      "Start ActualEndDate\n",
      "Start dxt_AliasName\n",
      "Start AliasSerNum\n",
      "Start CourseSerNum\n",
      "Start PlanSerNum\n",
      "Start TreatmentOrientation\n",
      "Start month\n",
      "Start week\n",
      "Start hour\n",
      "Start AppointmentSerNum\n",
      "Start numberical features\n",
      "\n",
      "Start age\n",
      "Start Scheduled_duration\n",
      "Start Actual_duration\n"
     ]
    }
   ],
   "source": [
    "new_appt = pd.DataFrame({})\n",
    "\n",
    "print('Start cateorical features')\n",
    "print('\\nStart PatientSerNum')\n",
    "new_appt['PatientSerNum'] = data_part1.groupby('AppointmentSerNum').PatientSerNum.apply(set)\n",
    "new_appt['PatientSerNum'] = new_appt['PatientSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start Sex')\n",
    "new_appt['Sex'] = data_part1.groupby('AppointmentSerNum').Sex.apply(set)\n",
    "new_appt['Sex'] = new_appt['Sex'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start DoctorSerNum')\n",
    "new_appt['DoctorSerNum'] = data_part1.groupby('AppointmentSerNum').DoctorSerNum.apply(set)\n",
    "new_appt['DoctorSerNum'] = new_appt['DoctorSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start date')\n",
    "new_appt['date'] = data_part1.groupby('AppointmentSerNum').date.apply(set)\n",
    "new_appt['date'] = new_appt['date'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ScheduledStartTime')\n",
    "new_appt['ScheduledStartTime'] = data_part1.groupby('AppointmentSerNum').ScheduledStartTime.apply(set)\n",
    "new_appt['ScheduledStartTime'] = new_appt['ScheduledStartTime'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ScheduledEndTime')\n",
    "new_appt['ScheduledEndTime'] = data_part1.groupby('AppointmentSerNum').ScheduledEndTime.apply(set)\n",
    "new_appt['ScheduledEndTime'] = new_appt['ScheduledEndTime'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ActualStartDate')\n",
    "new_appt['ActualStartDate'] = data_part1.groupby('AppointmentSerNum').ActualStartDate.apply(set)\n",
    "new_appt['ActualStartDate'] = new_appt['ActualStartDate'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ActualEndDate')\n",
    "new_appt['ActualEndDate'] = data_part1.groupby('AppointmentSerNum').ActualEndDate.apply(set)\n",
    "new_appt['ActualEndDate'] = new_appt['ActualEndDate'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start dxt_AliasName')\n",
    "new_appt['dxt_AliasName'] = data_part1.groupby('AppointmentSerNum').dxt_AliasName.apply(set)\n",
    "new_appt['dxt_AliasName'] = new_appt['dxt_AliasName'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start AliasSerNum')\n",
    "new_appt['AliasSerNum'] = data_part1.groupby('AppointmentSerNum').AliasSerNum.apply(set)\n",
    "new_appt['AliasSerNum'] = new_appt['AliasSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start CourseSerNum')\n",
    "new_appt['CourseSerNum'] = data_part1.groupby('AppointmentSerNum').CourseSerNum.apply(set)\n",
    "new_appt['CourseSerNum'] = new_appt['CourseSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start PlanSerNum')\n",
    "new_appt['PlanSerNum'] = data_part1.groupby('AppointmentSerNum').PlanSerNum.apply(set)\n",
    "new_appt['PlanSerNum'] = new_appt['PlanSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start TreatmentOrientation')\n",
    "new_appt['TreatmentOrientation'] = data_part1.groupby('AppointmentSerNum').TreatmentOrientation.apply(set)\n",
    "new_appt['TreatmentOrientation'] = new_appt['TreatmentOrientation'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start month')\n",
    "new_appt['month'] = data_part1.groupby('AppointmentSerNum').month.apply(set)\n",
    "new_appt['month'] = new_appt['month'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start week')\n",
    "new_appt['week'] = data_part1.groupby('AppointmentSerNum').week.apply(set)\n",
    "new_appt['week'] = new_appt['week'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start hour')\n",
    "new_appt['hour'] = data_part1.groupby('AppointmentSerNum').hour.apply(set)\n",
    "new_appt['hour'] = new_appt['hour'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start AppointmentSerNum')\n",
    "new_appt['AppointmentSerNum'] = new_appt.index.tolist()\n",
    "\n",
    "\n",
    "print('Start numberical features')\n",
    "print('\\nStart age')\n",
    "new_appt['age'] = data_part1.groupby('AppointmentSerNum').age.mean()\n",
    "\n",
    "print('Start Scheduled_duration')\n",
    "new_appt['Scheduled_duration'] = data_part1.groupby('AppointmentSerNum').Scheduled_duration.mean()\n",
    "\n",
    "print('Start Actual_duration')\n",
    "new_appt['Actual_duration'] = data_part1.groupby('AppointmentSerNum').Actual_duration.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_appt_ = new_appt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start categorical features\n",
      "\n",
      "Start FractionNumber\n",
      "Start UserName\n",
      "Start RadiationSerNum\n",
      "Start RadiationId\n",
      "Start ResourceSerNum\n",
      "Start CourseId\n",
      "Start PatientSerNum\n",
      "Start date\n",
      "Start numberical features\n",
      "\n",
      "Start ImagesTaken_total\n",
      "Start MU_total\n",
      "Start MUCoeff_total\n",
      "Start TreatmentTime_total\n"
     ]
    }
   ],
   "source": [
    "new_treat = pd.DataFrame({})\n",
    "\n",
    "print('Start categorical features')\n",
    "print('\\nStart FractionNumber')\n",
    "new_treat['FractionNumber'] = data_part2.groupby(['PatientSerNum', 'date']).FractionNumber.apply(set)\n",
    "new_treat['FractionNumber'] = new_treat['FractionNumber'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start UserName')\n",
    "new_treat['UserName'] = data_part2.groupby(['PatientSerNum', 'date']).UserName.apply(set)\n",
    "new_treat['UserName'] = new_treat['UserName'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start RadiationSerNum')\n",
    "new_treat['RadiationSerNum'] = data_part2.groupby(['PatientSerNum', 'date']).RadiationSerNum.apply(set)\n",
    "new_treat['RadiationSerNum'] = new_treat['RadiationSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start RadiationId')\n",
    "new_treat['RadiationId'] = data_part2.groupby(['PatientSerNum', 'date']).RadiationId.apply(set)\n",
    "new_treat['RadiationId'] = new_treat['RadiationId'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start ResourceSerNum')\n",
    "new_treat['ResourceSerNum'] = data_part2.groupby(['PatientSerNum', 'date']).ResourceSerNum.apply(set)\n",
    "new_treat['ResourceSerNum'] = new_treat['ResourceSerNum'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start CourseId')\n",
    "new_treat['CourseId'] = data_part2.groupby(['PatientSerNum', 'date']).CourseId.apply(set)\n",
    "new_treat['CourseId'] = new_treat['CourseId'].apply(lambda x: get_list(x))\n",
    "\n",
    "print('Start PatientSerNum')\n",
    "new_treat['PatientSerNum'] = new_treat.index.get_level_values(level = 0).tolist()\n",
    "\n",
    "print('Start date')\n",
    "new_treat['date'] = new_treat.index.get_level_values(level = 1).tolist()\n",
    "\n",
    "\n",
    "print('Start numberical features')\n",
    "print('\\nStart ImagesTaken_total')\n",
    "new_treat['ImagesTaken_total'] = data_part2.groupby(['PatientSerNum', 'date']).ImagesTaken.sum()\n",
    "\n",
    "print('Start MU_total')\n",
    "new_treat['MU_total'] = data_part2.groupby(['PatientSerNum', 'date']).MU.sum()\n",
    "\n",
    "print('Start MUCoeff_total')\n",
    "new_treat['MUCoeff_total'] = data_part2.groupby(['PatientSerNum', 'date']).MUCoeff.sum()\n",
    "\n",
    "print('Start TreatmentTime_total')\n",
    "new_treat['TreatmentTime_total'] = data_part2.groupby(['PatientSerNum', 'date']).TreatmentTime.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_treat_ = new_treat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_appt_.reset_index(drop = True, inplace = True)\n",
    "\n",
    "new_treat_.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PatientSerNum          int64\n",
       "AppointmentSerNum      int64\n",
       "ScheduledStartTime    object\n",
       "FractionNumber        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA[['PatientSerNum', 'AppointmentSerNum', 'ScheduledStartTime', 'FractionNumber']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of new_appt is (321034, 20)\n",
      "The shape of new_treat is (142974, 12)\n",
      "The shape of data is (139944, 30)\n"
     ]
    }
   ],
   "source": [
    "print(f'The shape of new_appt is {new_appt_.shape}')\n",
    "print(f'The shape of new_treat is {new_treat_.shape}')\n",
    "\n",
    "DATA_ = pd.merge(new_appt_, new_treat_, on = ['PatientSerNum', 'date'], how = 'inner')\n",
    "DATA_.sort_values(by = ['PatientSerNum', 'AppointmentSerNum', 'ScheduledStartTime'], inplace = True)\n",
    "print(f'The shape of data is {DATA_.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data is (126936, 30)\n"
     ]
    }
   ],
   "source": [
    "DATA = DATA_[(DATA_.Actual_duration >= 10) &\n",
    "             (DATA_.Actual_duration <= 60)]\n",
    "print(f'The shape of data is {DATA.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = DATA_.Actual_duration.tolist()\n",
    "y.sort()\n",
    "y[int(0.01 * len(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.0"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-int(0.01 * len(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series_seq 8101\n",
      "series_one 862\n"
     ]
    }
   ],
   "source": [
    "DATA_grouped = DATA.groupby('PatientSerNum')\n",
    "\n",
    "series_count = DATA_grouped.count()\n",
    "series_seq = series_count[series_count.AppointmentSerNum > 1].index.tolist()\n",
    "series_one = series_count[series_count.AppointmentSerNum == 1].index.tolist()\n",
    "print(f'series_seq {len(series_seq)}')\n",
    "print(f'series_one {len(series_one)}')\n",
    "\n",
    "series_seq = pd.DataFrame({'PatientSerNum': series_seq})\n",
    "series_data = pd.merge(series_seq, DATA, on = 'PatientSerNum', how = 'inner')\n",
    "series_data.sort_values(by = ['PatientSerNum', 'ScheduledStartTime'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of patient is 8101\n",
      "The max length of patient is 76\n",
      "The number of input encoded-feature is 441\n"
     ]
    }
   ],
   "source": [
    "series_data_grouped = series_data.groupby('PatientSerNum')\n",
    "pat_list = shuffle(list(series_data_grouped.groups.keys()), random_state = 1)\n",
    "\n",
    "APPT_LEN = series_data_grouped.count().AppointmentSerNum.max()\n",
    "FEATURE_LEN = 441\n",
    "print(f'The total number of patient is {len(pat_list)}')\n",
    "print(f'The max length of patient is {APPT_LEN}')\n",
    "print(f'The number of input encoded-feature is {FEATURE_LEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = series_data_grouped.get_group(pat_list[10])\n",
    "train_x, train_y, train_sample = generate_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(train_sample):# APPT_LEN, FEATURE_LEN\n",
    "    \n",
    "    # 样本标签\n",
    "    train_y = np.array([train_sample.Actual_duration.iloc[-1]])\n",
    "    \n",
    "    # 最后一个appointment 为我们需要预测的真实治疗时长所对应的appointment，所以需要设置为0\n",
    "    train_sample.Actual_duration.iloc[-1] = 0\n",
    "\n",
    "    # 因为存在相隔很远的两次预约，因此，构造特征Interval_scheduled 来度量两次预约之间的距离\n",
    "    ## 上一次的预期治疗开始时间\n",
    "    train_sample['Last_ScheduledStartTime'] = train_sample.ScheduledStartTime.shift(\n",
    "        periods = 1, fill_value = train_sample.ScheduledStartTime.iloc[0])\n",
    "    ## 需要将时间戳从字符型转为datetime 类型，从而计算时间间隔\n",
    "    train_sample['Last_ScheduledStartTime'] = train_sample.Last_ScheduledStartTime.apply(lambda x: str_to_Datetime(x))\n",
    "    train_sample['ScheduledStartTime'] = train_sample.ScheduledStartTime.apply(lambda x: str_to_Datetime(x))\n",
    "    ## 时间间隔\n",
    "    train_sample['Interval_scheduled'] = train_sample.apply(\n",
    "        lambda x: (x.ScheduledStartTime - x.Last_ScheduledStartTime).days, axis = 1)\n",
    "    \n",
    "    # 对分类变量进行one-hot encoding处理\n",
    "    encode_cate = pd.DataFrame({})\n",
    "    \n",
    "    # 这个地方将Sex 作为序列的一部分，并不是在最后的隐藏层进行合并\n",
    "    encode_cate['Sex'] = train_sample['Sex'].apply(\n",
    "        lambda x: sum(label_encoder_Sex.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.vstack(encode_cate.Sex.tolist())\n",
    "    \n",
    "    encode_cate['dxt_AliasName'] = train_sample['dxt_AliasName'].apply(\n",
    "        lambda x: sum(label_encoder_dxt_AliasName.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.dxt_AliasName.tolist())))\n",
    "\n",
    "    encode_cate['AliasSerNum'] = train_sample['AliasSerNum'].apply(\n",
    "        lambda x: sum(label_encoder_AliasSerNum.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.AliasSerNum.tolist())))\n",
    "\n",
    "    encode_cate['month'] = train_sample['month'].apply(\n",
    "        lambda x: sum(label_encoder_month.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.month.tolist())))\n",
    "\n",
    "    encode_cate['week'] = train_sample['week'].apply(\n",
    "        lambda x: sum(label_encoder_week.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.week.tolist())))\n",
    "\n",
    "    encode_cate['hour'] = train_sample['hour'].apply(\n",
    "        lambda x: sum(label_encoder_hour.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.hour.tolist())))\n",
    "\n",
    "    encode_cate['DoctorSerNum'] = train_sample['DoctorSerNum'].apply(\n",
    "        lambda x: sum(label_encoder_DoctorSerNum.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.DoctorSerNum.tolist())))\n",
    "\n",
    "    encode_cate['TreatmentOrientation'] = train_sample['TreatmentOrientation'].apply(\n",
    "        lambda x: sum(label_encoder_TreatmentOrientation.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.TreatmentOrientation.tolist())))\n",
    "\n",
    "    encode_cate['FractionNumber'] = train_sample['FractionNumber'].apply(\n",
    "        lambda x: sum(label_encoder_FractionNumber.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.FractionNumber.tolist())))\n",
    "\n",
    "    encode_cate['UserName'] = train_sample['UserName'].apply(\n",
    "        lambda x: sum(label_encoder_UserName.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.UserName.tolist())))\n",
    "\n",
    "    encode_cate['CourseId'] = train_sample['CourseId'].apply(\n",
    "        lambda x: sum(label_encoder_CourseId.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.CourseId.tolist())))\n",
    "\n",
    "    encode_cate['ResourceSerNum'] = train_sample['ResourceSerNum'].apply(\n",
    "        lambda x: sum(label_encoder_ResourceSerNum.transform(np.array(x).reshape(-1,1))))\n",
    "    train_x = np.hstack((train_x, np.vstack(encode_cate.ResourceSerNum.tolist())))\n",
    "    \n",
    "    # 将数值变量和非数值变量进行合并\n",
    "    train_num = train_sample[feature_num]\n",
    "    train_x = np.hstack((train_x, train_num))\n",
    "    \n",
    "#     # 需要满足序列长度的要求，因此对于短序列进行补零操作\n",
    "#     zeros = np.zeros((APPT_LEN - train_x.shape[0], FEATURE_LEN))\n",
    "#     train_x = np.vstack((zeros, train_x))\n",
    "    \n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 128)               291840    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 296,001\n",
      "Trainable params: 296,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 不指定时间戳长度\n",
    "model_sequence = Sequential()\n",
    "model_sequence.add(layers.LSTM(\n",
    "    batch_input_shape = (None , None, FEATURE_LEN),\n",
    "    output_dim = 128,\n",
    "    dropout=0.1,\n",
    "    recurrent_dropout=0.5,\n",
    "    return_sequences=False,\n",
    "))\n",
    "\n",
    "# model_sequence.add(layers.LSTM(\n",
    "#         output_dim = 32,\n",
    "#         ))\n",
    "# stateful = True 本次batch的参数返回到下一次的训练中\n",
    "\n",
    "model_sequence.add(layers.Dense(32))\n",
    "\n",
    "model_sequence.add(layers.Dense(1))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "model_sequence.compile(\n",
    "        optimizer = adam,\n",
    "        loss = 'mae'\n",
    "        )\n",
    "\n",
    "model_sequence.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_train = 40\n",
    "BATCH_test = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1.0 11.376006898880005\n",
      "\n",
      "Batch 2.0 7.1049095821380615\n",
      "\n",
      "Batch 3.0 6.585110454559326\n",
      "\n",
      "Batch 4.0 8.101087188720703\n",
      "\n",
      "Batch 5.0 5.264042434692382\n",
      "\n",
      "Batch 6.0 4.95974250793457\n",
      "\n",
      "Batch 7.0 5.982854022979736\n",
      "\n",
      "Batch 8.0 5.301941318511963\n",
      "\n",
      "Batch 9.0 7.220023775100708\n",
      "\n",
      "Batch 10.0 5.784545879364014\n",
      "\n",
      "Batch 11.0 4.61887035369873\n",
      "\n",
      "Batch 12.0 6.393277282714844\n",
      "\n",
      "Batch 13.0 4.73020378112793\n",
      "\n",
      "Batch 14.0 4.804464511871338\n",
      "\n",
      "Batch 15.0 5.209973964691162\n",
      "\n",
      "Batch 16.0 5.292972812652588\n",
      "\n",
      "Batch 17.0 6.862983932495117\n",
      "\n",
      "Batch 18.0 5.959686260223389\n",
      "\n",
      "Batch 19.0 6.032432594299316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-048495f359e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpat_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtrain_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_data_grouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain_x_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_x_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-224-3aa4ade1b6aa>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[1;34m(train_sample)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m## 上一次的预期治疗开始时间\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     train_sample['Last_ScheduledStartTime'] = train_sample.ScheduledStartTime.shift(\n\u001b[1;32m---> 12\u001b[1;33m         periods = 1, fill_value = train_sample.ScheduledStartTime.iloc[0])\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m## 需要将时间戳从字符型转为datetime 类型，从而计算时间间隔\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Last_ScheduledStartTime'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLast_ScheduledStartTime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr_to_Datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3369\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3370\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3450\u001b[0m         \u001b[1;31m# value exception to occur first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3452\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[1;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[0;32m   3244\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3245\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3246\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3247\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3248\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_result_batch = 0\n",
    "for i in range(len(pat_list)):\n",
    "    # 构造训练数据\n",
    "    pat = pat_list[i]\n",
    "    train_sample = series_data_grouped.get_group(pat)\n",
    "    train_x_i, train_y_i = generate_sample(train_sample)\n",
    "    \n",
    "    train_x = np.array([train_x_i])\n",
    "    train_y = np.array([train_y_i])\n",
    "    train_result = model_sequence.train_on_batch(train_x, train_y)\n",
    "    # print(train_result)\n",
    "    if (i+1)%50 == 0:\n",
    "        print(f'\\nBatch {(i+1)/50} {train_result_batch/50}')\n",
    "        train_result_batch = 0\n",
    "    else:\n",
    "        train_result_batch = train_result_batch + train_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-4b87a476e2e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpat_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtrain_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_data_grouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtrain_x_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAPPT_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFEATURE_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-7ce232b9a36c>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[1;34m(train_sample, APPT_LEN, FEATURE_LEN)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# 最后一个appointment 为我们需要预测的真实治疗时长所对应的appointment，所以需要设置为0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActual_duration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 因为存在相隔很远的两次预约，因此，构造特征Interval_scheduled 来度量两次预约之间的距离\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# check for chained assignment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;31m# actually do the set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3197\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m                 self._check_setitem_copy(stacklevel=4, t='referant',\n\u001b[1;32m-> 3199\u001b[1;33m                                          force=True)\n\u001b[0m\u001b[0;32m   3200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3201\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[1;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[0;32m   3244\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3245\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3246\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3247\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3248\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(int(len(pat_list)/(BATCH_train + BATCH_test))):\n",
    "    print(f'Batch {i}')\n",
    "    # 构造训练数据\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for tr in range((BATCH_train+BATCH_test)*i,\n",
    "                    (BATCH_train+BATCH_test)*(i+1) - BATCH_test):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        train_x_i, train_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        train_x.append(train_x_i)\n",
    "        train_y.append(train_y_i)\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "#     train_result = model_sequence.train_on_batch(train_x, train_y)\n",
    "        \n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    for te in range((BATCH_train+BATCH_test)*(i+1) - BATCH_test,\n",
    "                    (BATCH_train+BATCH_test)*(i+1)):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        val_x_i, val_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        val_x.append(val_x_i)\n",
    "        val_y.append(val_y_i)\n",
    "    val_x = np.array(val_x)\n",
    "    val_y = np.array(val_y)\n",
    "    break\n",
    "    # model_sequence.train_on_batch(val_x, val_y)\n",
    "    val_result = model_sequence.evaluate(val_x, val_y, verbose=0) # verbose 显示的时候有进度条\n",
    "    print(f'{tr+1 - BATCH_train}-{tr+1}-train [{train_result}] ||| {te+1 - BATCH_test}-{te+1}-validation [{val_result}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_5 (GRU)                  (None, 80, 128)           217728    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 32)                15456     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 233,217\n",
      "Trainable params: 233,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# dropout=0.1,\n",
    "# recurrent_dropout=0.5,\n",
    "\n",
    "model_sequence1 = Sequential()\n",
    "model_sequence1.add(layers.GRU(\n",
    "        batch_input_shape = (None , APPT_LEN, FEATURE_LEN),\n",
    "        output_dim = 128,\n",
    "\n",
    "        return_sequences=True,\n",
    "        ))\n",
    "\n",
    "# dropout=0.1,\n",
    "# recurrent_dropout=0.5,\n",
    "\n",
    "model_sequence1.add(layers.GRU(\n",
    "        output_dim = 32,\n",
    "\n",
    "        ))\n",
    "# stateful = True 本次batch的参数返回到下一次的训练中\n",
    "\n",
    "model_sequence1.add(layers.Dense(1))\n",
    "\n",
    "model_sequence1.compile(\n",
    "        optimizer = 'rmsprop',\n",
    "        loss = 'mae'\n",
    "        )\n",
    "\n",
    "model_sequence1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "0-40-train [29.033550262451172] ||| 40-45-validation [17.26938247680664]\n",
      "Batch 1\n",
      "45-85-train [18.42552375793457] ||| 85-90-validation [15.728889465332031]\n",
      "Batch 2\n",
      "90-130-train [17.172286987304688] ||| 130-135-validation [10.300146102905273]\n",
      "Batch 3\n",
      "135-175-train [20.477676391601562] ||| 175-180-validation [20.931716918945312]\n",
      "Batch 4\n",
      "180-220-train [15.885931015014648] ||| 220-225-validation [14.639741897583008]\n",
      "Batch 5\n",
      "225-265-train [19.633190155029297] ||| 265-270-validation [7.4754228591918945]\n",
      "Batch 6\n",
      "270-310-train [13.881879806518555] ||| 310-315-validation [20.360618591308594]\n",
      "Batch 7\n",
      "315-355-train [14.46210765838623] ||| 355-360-validation [3.0205907821655273]\n",
      "Batch 8\n",
      "360-400-train [12.831311225891113] ||| 400-405-validation [14.861434936523438]\n",
      "Batch 9\n",
      "405-445-train [14.254976272583008] ||| 445-450-validation [5.8323445320129395]\n",
      "Batch 10\n",
      "450-490-train [12.768884658813477] ||| 490-495-validation [6.7624006271362305]\n",
      "Batch 11\n",
      "495-535-train [12.343925476074219] ||| 535-540-validation [16.65709686279297]\n",
      "Batch 12\n",
      "540-580-train [13.27532958984375] ||| 580-585-validation [3.600989580154419]\n",
      "Batch 13\n",
      "585-625-train [13.190481185913086] ||| 625-630-validation [14.562875747680664]\n",
      "Batch 14\n",
      "630-670-train [15.462725639343262] ||| 670-675-validation [5.492772102355957]\n",
      "Batch 15\n",
      "675-715-train [10.875143051147461] ||| 715-720-validation [7.449995517730713]\n",
      "Batch 16\n",
      "720-760-train [13.16185474395752] ||| 760-765-validation [5.439629077911377]\n",
      "Batch 17\n",
      "765-805-train [9.466924667358398] ||| 805-810-validation [9.367010116577148]\n",
      "Batch 18\n",
      "810-850-train [11.547209739685059] ||| 850-855-validation [33.32210159301758]\n",
      "Batch 19\n",
      "855-895-train [12.11719036102295] ||| 895-900-validation [11.279465675354004]\n",
      "Batch 20\n",
      "900-940-train [16.106698989868164] ||| 940-945-validation [3.7650279998779297]\n",
      "Batch 21\n",
      "945-985-train [9.322242736816406] ||| 985-990-validation [43.19862747192383]\n",
      "Batch 22\n",
      "990-1030-train [8.37379264831543] ||| 1030-1035-validation [3.158372402191162]\n",
      "Batch 23\n",
      "1035-1075-train [10.975160598754883] ||| 1075-1080-validation [3.114107608795166]\n",
      "Batch 24\n",
      "1080-1120-train [9.563238143920898] ||| 1120-1125-validation [6.077256679534912]\n",
      "Batch 25\n",
      "1125-1165-train [9.887956619262695] ||| 1165-1170-validation [14.038228988647461]\n",
      "Batch 26\n",
      "1170-1210-train [10.549654006958008] ||| 1210-1215-validation [8.002077102661133]\n",
      "Batch 27\n",
      "1215-1255-train [12.93274211883545] ||| 1255-1260-validation [4.969959259033203]\n",
      "Batch 28\n",
      "1260-1300-train [10.397086143493652] ||| 1300-1305-validation [10.935868263244629]\n",
      "Batch 29\n",
      "1305-1345-train [18.499923706054688] ||| 1345-1350-validation [15.90489673614502]\n",
      "Batch 30\n",
      "1350-1390-train [7.554978847503662] ||| 1390-1395-validation [7.912192344665527]\n",
      "Batch 31\n",
      "1395-1435-train [8.987907409667969] ||| 1435-1440-validation [3.1541738510131836]\n",
      "Batch 32\n",
      "1440-1480-train [9.405927658081055] ||| 1480-1485-validation [3.812253952026367]\n",
      "Batch 33\n",
      "1485-1525-train [11.791290283203125] ||| 1525-1530-validation [4.779034614562988]\n",
      "Batch 34\n",
      "1530-1570-train [8.295125961303711] ||| 1570-1575-validation [3.750781297683716]\n",
      "Batch 35\n",
      "1575-1615-train [9.425666809082031] ||| 1615-1620-validation [1.7165288925170898]\n",
      "Batch 36\n",
      "1620-1660-train [10.703584671020508] ||| 1660-1665-validation [10.684527397155762]\n",
      "Batch 37\n",
      "1665-1705-train [8.683987617492676] ||| 1705-1710-validation [1.1592350006103516]\n",
      "Batch 38\n",
      "1710-1750-train [9.088254928588867] ||| 1750-1755-validation [1.3807296752929688]\n",
      "Batch 39\n",
      "1755-1795-train [8.167673110961914] ||| 1795-1800-validation [0.41464900970458984]\n",
      "Batch 40\n",
      "1800-1840-train [9.455282211303711] ||| 1840-1845-validation [14.549215316772461]\n",
      "Batch 41\n",
      "1845-1885-train [8.640848159790039] ||| 1885-1890-validation [3.517909288406372]\n",
      "Batch 42\n",
      "1890-1930-train [8.108800888061523] ||| 1930-1935-validation [12.482658386230469]\n",
      "Batch 43\n",
      "1935-1975-train [9.83915901184082] ||| 1975-1980-validation [0.45415210723876953]\n",
      "Batch 44\n",
      "1980-2020-train [11.786384582519531] ||| 2020-2025-validation [8.418599128723145]\n",
      "Batch 45\n",
      "2025-2065-train [7.060329437255859] ||| 2065-2070-validation [0.3853330612182617]\n",
      "Batch 46\n",
      "2070-2110-train [8.810149192810059] ||| 2110-2115-validation [4.351316452026367]\n",
      "Batch 47\n",
      "2115-2155-train [9.50554370880127] ||| 2155-2160-validation [0.6827058792114258]\n",
      "Batch 48\n",
      "2160-2200-train [9.206945419311523] ||| 2200-2205-validation [9.284711837768555]\n",
      "Batch 49\n",
      "2205-2245-train [8.41862964630127] ||| 2245-2250-validation [10.249488830566406]\n",
      "Batch 50\n",
      "2250-2290-train [12.751091003417969] ||| 2290-2295-validation [38.21573257446289]\n",
      "Batch 51\n",
      "2295-2335-train [8.283462524414062] ||| 2335-2340-validation [2.177316665649414]\n",
      "Batch 52\n",
      "2340-2380-train [14.52015209197998] ||| 2380-2385-validation [0.7885618209838867]\n",
      "Batch 53\n",
      "2385-2425-train [10.61923885345459] ||| 2425-2430-validation [1.1009511947631836]\n",
      "Batch 54\n",
      "2430-2470-train [9.967939376831055] ||| 2470-2475-validation [35.06837463378906]\n",
      "Batch 55\n",
      "2475-2515-train [9.154203414916992] ||| 2515-2520-validation [3.0340394973754883]\n",
      "Batch 56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c7e44abeab68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpat_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtrain_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_data_grouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtrain_x_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAPPT_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFEATURE_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-930521d87647>\u001b[0m in \u001b[0;36mgenerate_sample\u001b[1;34m(train_sample, APPT_LEN, FEATURE_LEN)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m## 时间间隔\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     train_sample['Interval_scheduled'] = train_sample.apply(\n\u001b[1;32m---> 18\u001b[1;33m         lambda x: (x.ScheduledStartTime - x.Last_ScheduledStartTime).days, axis = 1)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# 对分类变量进行one-hot encoding处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3369\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3370\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3450\u001b[0m         \u001b[1;31m# value exception to occur first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3452\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[1;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[0;32m   3244\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3245\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3246\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3247\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3248\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "for i in range(int(len(pat_list)/(BATCH_train + BATCH_test))):\n",
    "    print(f'Batch {i}')\n",
    "    # 构造训练数据\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for tr in range((BATCH_train+BATCH_test)*i,\n",
    "                    (BATCH_train+BATCH_test)*(i+1) - BATCH_test):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        train_x_i, train_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        train_x.append(train_x_i)\n",
    "        train_y.append(train_y_i)\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    train_result = model_sequence1.train_on_batch(train_x, train_y)\n",
    "        \n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    for te in range((BATCH_train+BATCH_test)*(i+1) - BATCH_test,\n",
    "                    (BATCH_train+BATCH_test)*(i+1)):\n",
    "        pat = pat_list[tr]\n",
    "        train_sample = series_data_grouped.get_group(pat)\n",
    "        val_x_i, val_y_i = generate_sample(train_sample, APPT_LEN, FEATURE_LEN)\n",
    "        val_x.append(val_x_i)\n",
    "        val_y.append(val_y_i)\n",
    "    val_x = np.array(val_x)\n",
    "    val_y = np.array(val_y)\n",
    "    # model_sequence.train_on_batch(val_x, val_y)\n",
    "    val_result = model_sequence1.evaluate(val_x, val_y, verbose=0) # verbose 显示的时候有进度条\n",
    "    print(f'{tr+1 - BATCH_train}-{tr+1}-train [{train_result}] ||| {te+1 - BATCH_test}-{te+1}-validation [{val_result}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
