{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append(r'D:\\jupyter files\\waiting_time_project\\my_tools')\n",
    "import tools_for_os.for_df as ml_df\n",
    "import tools_for_os.for_file as ml_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\jupyter files\\data_waiting_time_project\\preprocess_data\\lightgbm_data\\ already existed!\n"
     ]
    }
   ],
   "source": [
    "data_path = 'D:\\\\jupyter files\\\\data_waiting_time_project\\\\preprocess_data\\\\'\n",
    "lightgbm_data_path = data_path + 'lightgbm_data\\\\'\n",
    "\n",
    "ml_fl.create_folder(lightgbm_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_appt_info():\n",
    "    print('='*40)\n",
    "    print(f'Start preprocessing data about appointment information')\n",
    "    \n",
    "    print('='*20)\n",
    "    print(f'Get data from mysql')\n",
    "    sql_appointment = \"\"\"SELECT * FROM appointment\"\"\"\n",
    "    data_appointment = ml_df.get_df_from_sql(sql_appointment)\n",
    "    print(f'\\nThe shape of data_appointment {data_appointment.shape}')\n",
    "    sql_pt = \"\"\"SELECT * FROM patient\"\"\"\n",
    "    data_pt = ml_df.get_df_from_sql(sql_pt)\n",
    "    print(f'The shape of data_pt {data_pt.shape}')\n",
    "    sql_ptc = \"\"\"SELECT * FROM patientcopy\"\"\"\n",
    "    data_ptc = ml_df.get_df_from_sql(sql_ptc)\n",
    "    print(f'The shape of data_ptc {data_ptc.shape}')\n",
    "    sql_pd = \"\"\"SELECT * FROM patientdoctor\"\"\"\n",
    "    data_pd = ml_df.get_df_from_sql(sql_pd)\n",
    "    print(f'The shape of data_pd {data_pd.shape}')\n",
    "    sql_dx = \"\"\"SELECT * FROM diagnosis\"\"\"\n",
    "    data_dx = ml_df.get_df_from_sql(sql_dx)\n",
    "    print(f'The shape of data_dx {data_dx.shape}')\n",
    "    sql_dxt = \"\"\"SELECT * FROM diagnosistranslation\"\"\"\n",
    "    data_dxt = ml_df.get_df_from_sql(sql_dxt)\n",
    "    print(f'The shape of data_dxt {data_dxt.shape}')\n",
    "    sql_co = \"\"\"SELECT * FROM course\"\"\"\n",
    "    data_co = ml_df.get_df_from_sql(sql_co)\n",
    "    print(f'The shape of data_co {data_co.shape}')\n",
    "    sql_pl = \"\"\"SELECT * FROM plan\"\"\"\n",
    "    data_pl = ml_df.get_df_from_sql(sql_pl)\n",
    "    print(f'The shape of data_pl {data_pl.shape}')\n",
    "    \n",
    "    try:\n",
    "        print('Drop columns')\n",
    "        data_appointment.drop(columns = ['LastUpdated'], inplace = True)\n",
    "        data_pt.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_ptc.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_pd.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_dx.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_dxt.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_co.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_pl.drop('LastUpdated', axis = 1, inplace = True)\n",
    "    except:\n",
    "        print('Finish droppping columns')\n",
    "    \n",
    "    print('\\nFinish getting data')\n",
    "    \n",
    "    print('='*20)\n",
    "    print(f'Start merging data')\n",
    "\n",
    "    # 筛选appointment 中的数据\n",
    "    print('\\nProcess data_appointment')\n",
    "    print(f'\\nThe shape of data_appointment {data_appointment.shape}')\n",
    "    data_appointment = data_appointment[((data_appointment.AliasSerNum == 31) |\n",
    "                                         (data_appointment.AliasSerNum == 23)) &\n",
    "                                         (data_appointment.ActualStartDate != datetime.datetime(1970, 1, 1, 0, 0)) &\n",
    "                                         (data_appointment.ActualEndDate != datetime.datetime(1970, 1, 1, 0, 0)) &\n",
    "                                         (data_appointment.ActualStartDate != data_appointment.ActualEndDate)]\n",
    "    print(f'The shape of data_appointment {data_appointment.shape}')\n",
    "\n",
    "    data_appointment = data_appointment[(data_appointment.State == 'Active') &\n",
    "                                          ((data_appointment.Status == 'Completed') |\n",
    "                                           (data_appointment.Status == 'Pt. CompltFinish'))]\n",
    "    print(f'The shape of data_appointment {data_appointment.shape}')\n",
    "\n",
    "    # 拼接data_pt 和data_appointment\n",
    "    print(f'\\nMerge data_pt and data_appointment')\n",
    "    pt_appt = pd.merge(data_appointment, data_pt, on = 'PatientSerNum', how = 'inner')\n",
    "    print(f'pt_appt shape is {pt_appt.shape}')\n",
    "    del data_appointment, data_pt\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'\\nMerge data_ptc')\n",
    "    pt_appt = pd.merge(pt_appt, data_ptc, on = 'PatientSerNum', how = 'inner')\n",
    "    print(f'pt_appt shape is {pt_appt.shape}')\n",
    "    del data_ptc\n",
    "    gc.collect()\n",
    "    \n",
    "    # PatientDoctor 的预处理\n",
    "    print('\\nProcess data_pd')\n",
    "    data_pd = data_pd[(data_pd.OncologistFlag == 1) &\n",
    "                       (data_pd.PrimaryFlag == 1)]\n",
    "    print(f'The shape of data_pd {data_pd.shape}')\n",
    "    \n",
    "    # 拼接pt_appt data_pd\n",
    "    print(f'\\nMerge data_pd_ and pt_appt')\n",
    "    # data_pd_, data_appt AliasSerNum 不同，data_appt_ 中只保留了23、31，data_pd_ 中只有37\n",
    "    data_pd.drop('AliasSerNum', axis = 1, inplace = True)\n",
    "    pt_pd_appt = pd.merge(pt_appt, data_pd, on = 'PatientSerNum', how = 'left')\n",
    "    print(f'\\npt_pd_appt shape is {pt_pd_appt.shape}')\n",
    "    del pt_appt, data_pd\n",
    "    gc.collect()\n",
    "\n",
    "    # 拼接data_dxt 和data_dx\n",
    "    print(f'\\nMerge data_dxt and data_dx')\n",
    "    data_dxt = data_dxt.rename(columns={'AliasName': 'dxt_AliasName'}, inplace = False)\n",
    "    dx_dxt = pd.merge(data_dx, data_dxt, on = 'DiagnosisCode', how = 'left')\n",
    "    print(f'dx_dxt shape is {dx_dxt.shape}')\n",
    "    del data_dx, data_dxt\n",
    "    gc.collect()    \n",
    "    \n",
    "    # 拼接pt_pd_appt 和dx_dxt\n",
    "    print(f'\\nMerge pt_pd_appt and dx_dxt')\n",
    "    dx_dxt.drop('AliasSerNum', axis = 1, inplace = True)\n",
    "    dx_dxt.drop('PatientSerNum', axis = 1, inplace = True)\n",
    "    pt_pd_appt_dx_dxt = pd.merge(pt_pd_appt, dx_dxt, on = ['DiagnosisSerNum'], how = 'inner')\n",
    "    print(f'pt_pd_appt_dx_dxt shape is {pt_pd_appt_dx_dxt.shape}')\n",
    "    del pt_pd_appt, dx_dxt\n",
    "    gc.collect()  \n",
    "    \n",
    "    # Plan 的预处理\n",
    "    print('\\nProcess data_pl')\n",
    "    data_pl = data_pl[data_pl.Status == 'TreatApproval']\n",
    "    data_pl.TreatmentOrientation = data_pl.TreatmentOrientation.apply(\n",
    "        lambda x: 'NULL' if x == '' else x)\n",
    "    print(f'The shape of data_pl {data_pl.shape}')\n",
    "\n",
    "    # 拼接data_pl_ 和data_co\n",
    "    print(f'\\nMerge data_pl and data_co')\n",
    "    data_pl.drop('AliasSerNum', axis = 1, inplace = True)\n",
    "    data_co.drop('AliasSerNum', axis = 1, inplace = True)\n",
    "    pl_co = pd.merge(data_pl, data_co, on = 'CourseSerNum', how = 'inner')\n",
    "    print(f'pl_co shape is {pl_co.shape}')\n",
    "    del data_pl, data_co\n",
    "    gc.collect()\n",
    "\n",
    "    print(f'\\nMerge pt_pd_appt_dx_dxt and pl_co')\n",
    "    pl_co.drop('AliasExpressionSerNum', axis = 1, inplace = True)\n",
    "    pl_co.drop('DiagnosisSerNum', axis = 1, inplace = True)\n",
    "    pl_co.drop('PrioritySerNum', axis = 1, inplace = True)\n",
    "    pl_co.rename(columns={'Status': 'Status_plan'}, inplace=True)\n",
    "    data_appt_info = pd.merge(pt_pd_appt_dx_dxt, pl_co,\n",
    "                              on = ['PatientSerNum'], \n",
    "                              how = 'inner')\n",
    "    print(f'\\ndata_appt_info shape is {data_appt_info.shape}')\n",
    "    del pt_pd_appt_dx_dxt, pl_co\n",
    "    gc.collect()\n",
    "    \n",
    "    print('\\nFinish getting data')\n",
    "    \n",
    "    #　删除值完全相同的列\n",
    "    print('='*20)\n",
    "    print(f'Drop columns with same values')\n",
    "    for col in data_appt_info.columns:\n",
    "        if len(data_appt_info[col].unique()) == 1:\n",
    "            data_appt_info.drop(col, axis = 1, inplace = True)\n",
    "    print(f'\\ndata_appt_info shape {data_appt_info.shape}')\n",
    "    \n",
    "    print('\\nFinish dropping')\n",
    "    \n",
    "    print('='*20)\n",
    "    print(f'Create features')\n",
    "    \n",
    "    feature_columns = [\n",
    "        'dxt_AliasName',\n",
    "        'DateOfBirth', 'Sex',\n",
    "        'AliasSerNum', 'PatientSerNum', 'AppointmentSerNum',\n",
    "        'ScheduledStartTime', 'ScheduledEndTime', 'ActualStartDate', 'ActualEndDate',\n",
    "        'CourseSerNum', \n",
    "        'DoctorSerNum',\n",
    "        'PlanSerNum',\n",
    "        'TreatmentOrientation'\n",
    "    ]\n",
    "\n",
    "    data_appt_info = data_appt_info[feature_columns]\n",
    "    \n",
    "    print(f'\\n Patients age')\n",
    "    # 患者年龄\n",
    "    data_appt_info['age'] = data_appt_info.apply(lambda x: int((x.ActualStartDate - x.DateOfBirth).days/365), axis = 1)\n",
    "    \n",
    "    print(f'\\n Month Date Week Hour')\n",
    "    # 时间相关的特征，月-日-周-小时\n",
    "    data_appt_info['month'] = data_appt_info.apply(lambda x: x.ScheduledStartTime.strftime(\"%m\"), axis = 1)\n",
    "    data_appt_info['date'] = data_appt_info.apply(lambda x: x.ScheduledStartTime.strftime(\"%Y--%m--%d\"), axis = 1)\n",
    "    data_appt_info['week'] = data_appt_info.apply(lambda x: x.ScheduledStartTime.strftime(\"%w\"), axis = 1)\n",
    "    data_appt_info['hour'] = data_appt_info.apply(lambda x: x.ScheduledStartTime.strftime(\"%H\"), axis = 1)\n",
    "    \n",
    "    print(f'\\n Two type Duration')\n",
    "    # 时长相关特征\n",
    "    data_appt_info['Scheduled_duration'] = data_appt_info.apply(lambda x: \n",
    "                                                                (x.ScheduledEndTime - x.ScheduledStartTime).seconds/60, axis = 1)\n",
    "    data_appt_info['Actual_duration'] = data_appt_info.apply(lambda x: \n",
    "                                                                       (x.ActualEndDate - x.ActualStartDate).seconds/60, axis = 1)\n",
    "\n",
    "    data_appt_info.sort_values(by = ['PatientSerNum', 'AppointmentSerNum'], inplace = True)\n",
    "\n",
    "    return data_appt_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_treat_info():\n",
    "    print('='*40)\n",
    "    print(f'Start preprocessing data about treatment information')\n",
    "    \n",
    "    print('='*20)\n",
    "    print(f'Get data from mysql')\n",
    "    sql_radiation = \"\"\"SELECT * FROM radiation\"\"\"\n",
    "    data_radiation = ml_df.get_df_from_sql(sql_radiation)\n",
    "    print(f'The shape of data_radiation {data_radiation.shape}')   \n",
    "    sql_radiationhstry = \"\"\"SELECT * FROM radiationhstry\"\"\"\n",
    "    data_radiationhstry = ml_df.get_df_from_sql(sql_radiationhstry)\n",
    "    print(f'The shape of data_radiationhstry {data_radiationhstry.shape}')\n",
    "    sql_plan = \"\"\"SELECT * FROM plan\"\"\"\n",
    "    data_plan = ml_df.get_df_from_sql(sql_plan)\n",
    "    print(f'The shape of data_plan {data_plan.shape}')\n",
    "    sql_course = \"\"\"SELECT * FROM course\"\"\"\n",
    "    data_course = ml_df.get_df_from_sql(sql_course)\n",
    "    print(f'The shape of data_course {data_course.shape}')\n",
    "    sql_patient = \"\"\"SELECT * FROM patient\"\"\"\n",
    "    data_patient = ml_df.get_df_from_sql(sql_patient)\n",
    "    print(f'The shape of data_patient {data_patient.shape}')\n",
    "    \n",
    "    try:\n",
    "        print('Drop columns')\n",
    "        data_radiation.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_radiationhstry.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_plan.drop('LastUpdated', axis = 1, inplace = True)\n",
    "        data_course.drop(columns = ['LastUpdated'], inplace = True)\n",
    "        data_patient.drop('LastUpdated', axis = 1, inplace = True)\n",
    "    except:\n",
    "        print('Finish droppping columns')\n",
    "    \n",
    "    print('\\nFinish getting data')\n",
    "    \n",
    "    print('='*20)\n",
    "    print(f'Start merging data')\n",
    "    \n",
    "    # Radiation 的预处理\n",
    "    print('\\nProcess data_radiation')\n",
    "    data_radiation = data_radiation[(data_radiation.DeliveryType == 'TREATMENT') &\n",
    "                                    (data_radiation.MU > 0) &\n",
    "                                    (data_radiation.MUCoeff > 0)]\n",
    "    print(f'The shape of data_radiation {data_radiation.shape}')\n",
    "\n",
    "    # Radiationhstry 的预处理\n",
    "    print('\\nProcess data_radiationhstry')\n",
    "    data_radiationhstry = data_radiationhstry[data_radiationhstry.TreatmentStartTime > pd.Timestamp('2015-01-01 00:00:00')]\n",
    "    print(f'The shape of data_radiationhstry {data_radiationhstry.shape}')\n",
    "\n",
    "    print(f'\\nMerge data_radiation and data_radiationhstry')\n",
    "    ra_rh = pd.merge(data_radiation, data_radiationhstry, \n",
    "                     on = ['RadiationSerNum', 'AliasSerNum'], how = 'inner')\n",
    "    print(f'The shape of ra_rh {ra_rh.shape}')\n",
    "    del data_radiation, data_radiationhstry\n",
    "    gc.collect\n",
    "\n",
    "    print(f'\\nMerge data_plan and ra_rh')\n",
    "    ra_rh_pl = pd.merge(data_plan, ra_rh, on = ['PlanSerNum', 'AliasSerNum'], how = 'inner')\n",
    "    print(f'The shape of ra_rh_pl {ra_rh_pl.shape}')\n",
    "    del data_plan, ra_rh\n",
    "    gc.collect()\n",
    "\n",
    "    print(f'\\nMerge data_course and data_patient')\n",
    "    co_pa = pd.merge(data_course, data_patient, on = 'PatientSerNum', how = 'inner')\n",
    "    print(f'The shape of co_pa {co_pa.shape}')\n",
    "    del data_course, data_patient\n",
    "    gc.collect()\n",
    "\n",
    "    print(f'\\nMerge co_pa and ra_rh_pl')\n",
    "    data_treat_info = pd.merge(co_pa, ra_rh_pl, \n",
    "                               on = ['CourseSerNum', 'AliasSerNum'], how = 'inner')\n",
    "    print(f'The shape of data_treat_info {data_treat_info.shape}')\n",
    "    del co_pa, ra_rh_pl\n",
    "    gc.collect()   \n",
    "\n",
    "    #　删除值完全相同的列\n",
    "    print('='*20)\n",
    "    print(f'Drop columns with same values')\n",
    "    for col in data_treat_info.columns:\n",
    "        if len(data_treat_info[col].unique()) == 1:\n",
    "            data_treat_info.drop(col, axis = 1, inplace = True)\n",
    "    print(f'\\ndata_treat_info shape {data_treat_info.shape}')\n",
    "    \n",
    "    print('\\nFinish dropping')\n",
    "    \n",
    "    print('='*20)\n",
    "    print(f'Create features')\n",
    "    \n",
    "    feature_columns = [\n",
    "        'RadiationHstryAriaSer', 'TreatmentStartTime', 'TreatmentEndTime', 'FractionNumber', 'ImagesTaken', 'UserName',\n",
    "        'RadiationSerNum', 'RadiationId', 'ResourceSerNum', 'MU', 'MUCoeff', 'TreatmentTime',\n",
    "        'PatientSerNum',\n",
    "        'CourseId'\n",
    "    ]\n",
    "\n",
    "    data_treat_info = data_treat_info[feature_columns]\n",
    "    data_treat_info['date'] = data_treat_info.apply(lambda x: x.TreatmentStartTime.strftime(\"%Y--%m--%d\"), axis = 1)\n",
    "    data_treat_info['Treatment_duration'] = data_treat_info.apply(lambda x: \n",
    "                                                                  (x.TreatmentEndTime - x.TreatmentStartTime).seconds, axis = 1)\n",
    "    data_treat_info.sort_values(by = ['PatientSerNum', 'RadiationHstryAriaSer'], inplace = True)\n",
    "    \n",
    "    return data_treat_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_xy(data_appt_info, data_treat_info):\n",
    "    print('='*40)\n",
    "    print('Start data preprocess to generate x y')\n",
    "    \n",
    "    print('='*20)\n",
    "    print(f'Merge data_appt_info, data_treat_info')\n",
    "    DATA = pd.merge(data_appt_info, data_treat_info, on = ['PatientSerNum', 'date'], how = 'inner')\n",
    "    DATA.sort_values(by = ['PatientSerNum', 'AppointmentSerNum', 'ScheduledStartTime', 'FractionNumber'], inplace = True)\n",
    "    \n",
    "    feature_num = ['Scheduled_duration', 'Actual_duration',\n",
    "                   'age', 'TreatmentTime', 'ImagesTaken',\n",
    "                   'MU', 'MUCoeff']\n",
    "    \n",
    "#     date\n",
    "    feature_cate = ['dxt_AliasName', 'Sex', 'AliasSerNum', \n",
    "                    'month', 'week', 'hour', 'date','DoctorSerNum', \n",
    "                    'TreatmentOrientation', 'FractionNumber',\n",
    "                    'UserName', 'RadiationId', 'CourseId', 'ResourceSerNum']\n",
    "    \n",
    "    DATA = DATA[feature_num + feature_cate]\n",
    "    DATA.drop_duplicates(inplace = True)\n",
    "    print(f'\\nThe shape of data is {DATA.shape}')\n",
    "    \n",
    "    print('\\nFinish merging')\n",
    "    \n",
    "    \n",
    "    # 数值变量暂时不做处理\n",
    "    print('='*20)\n",
    "    print(f'Preprocess numberical data')\n",
    "    \n",
    "    data_num = DATA[feature_num]\n",
    "    data_num.fillna(0, inplace = True)\n",
    "    # data_num = log1p(data_num)\n",
    "    \n",
    "    print(f'\\nFinish preprocessing')\n",
    "    \n",
    "    # 非数值变量进行categorical encoding\n",
    "    print('='*20)\n",
    "    print(f'Preprocess categorical data')\n",
    "    \n",
    "    data_cate_display = DATA[feature_cate]\n",
    "    data_cate_display.fillna('NaN', inplace = True)\n",
    "    data_cate_display = data_cate_display.astype(str)\n",
    "    \n",
    "    # OrdinalEncoder 处理非数值变量\n",
    "    print(f'\\nEncoding features')\n",
    "    feature_encoder = preprocessing.OrdinalEncoder()\n",
    "    feature_encoder.fit(data_cate_display.values)\n",
    "    data_cate = feature_encoder.transform(data_cate_display.values)\n",
    "\n",
    "    # array 2 dataframe\n",
    "    data_cate = pd.DataFrame(data_cate, columns=data_cate_display.columns)\n",
    "    # 还需要换成category 格式，这样lightgbm 会按照类别变量的方式进行处理\n",
    "    data_cate = data_cate.astype('category')\n",
    "    \n",
    "    print(f'\\nFinish preprocessing')\n",
    "    \n",
    "    \n",
    "    # 拼接数据\n",
    "    print('='*20)\n",
    "    print('Concat data_num and data_cate')\n",
    "    data_num.reset_index(drop=True, inplace=True)\n",
    "    data_cate_display.reset_index(drop=True, inplace=True)\n",
    "    data_display = pd.concat([data_num, data_cate_display], axis=1)\n",
    "\n",
    "    data_num.reset_index(drop=True, inplace=True)\n",
    "    data_cate.reset_index(drop=True, inplace=True)\n",
    "    data = pd.concat([data_num, data_cate], axis=1)\n",
    "    \n",
    "    print('\\nFinish concating')\n",
    "    \n",
    "    \n",
    "    print('='*20)\n",
    "    print('Get train set and test set')\n",
    "    data = data.sample(frac = 1, random_state = 1)\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    label = data[['Actual_duration']]\n",
    "    data.drop(['Actual_duration'], axis = 1, inplace = True)\n",
    "    \n",
    "    train_x = data.iloc[: int(data.shape[0] * 0.9)]\n",
    "    train_y = label.iloc[: int(data.shape[0] * 0.9)]\n",
    "    print(f'\\nThe number of train set is {train_x.shape[0]}')\n",
    "    \n",
    "    test_x = data.iloc[int(data.shape[0] * 0.9) :]\n",
    "    test_y = label.iloc[int(data.shape[0] * 0.9): ]\n",
    "    print(f'The number of test set is {test_x.shape[0]}')\n",
    "    \n",
    "    print('Finish getting')\n",
    "    \n",
    "    return data_display, data, train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(pred_y, test_y):\n",
    "    sq = sum((pred_y - test_y)**2) / len(pred_y)\n",
    "#     sq = sum((pred_y - np.array(test_y.Actual_duration.tolist()))**2) / len(pred_y)\n",
    "    rmse = np.sqrt(sq)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(pred_y, test_y):\n",
    "    mae = sum(np.abs((pred_y - test_y))) / len(pred_y)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Start preprocessing data about appointment information\n",
      "====================\n",
      "Get data from mysql\n",
      "\n",
      "The shape of data_appointment (1074478, 17)\n",
      "The shape of data_pt (51062, 2)\n",
      "The shape of data_ptc (51308, 6)\n",
      "The shape of data_pd (148770, 7)\n",
      "The shape of data_dx (39486, 12)\n",
      "The shape of data_dxt (1291, 6)\n",
      "The shape of data_co (58360, 11)\n",
      "The shape of data_pl (148421, 13)\n",
      "Drop columns\n",
      "\n",
      "Finish getting data\n",
      "====================\n",
      "Start merging data\n",
      "\n",
      "Process data_appointment\n",
      "\n",
      "The shape of data_appointment (1074478, 16)\n",
      "The shape of data_appointment (367413, 16)\n",
      "The shape of data_appointment (367410, 16)\n",
      "\n",
      "Merge data_pt and data_appointment\n",
      "pt_appt shape is (367410, 16)\n",
      "\n",
      "Merge data_ptc\n",
      "pt_appt shape is (367410, 20)\n",
      "\n",
      "Process data_pd\n",
      "The shape of data_pd (49874, 6)\n",
      "\n",
      "Merge data_pd_ and pt_appt\n",
      "\n",
      "pt_pd_appt shape is (410947, 24)\n",
      "\n",
      "Merge data_dxt and data_dx\n",
      "dx_dxt shape is (51712, 15)\n",
      "\n",
      "Merge pt_pd_appt and dx_dxt\n",
      "pt_pd_appt_dx_dxt shape is (459642, 36)\n",
      "\n",
      "Process data_pl\n",
      "The shape of data_pl (47300, 12)\n",
      "\n",
      "Merge data_pl and data_co\n",
      "pl_co shape is (47300, 19)\n",
      "\n",
      "Merge pt_pd_appt_dx_dxt and pl_co\n",
      "\n",
      "data_appt_info shape is (1047176, 51)\n",
      "\n",
      "Finish getting data\n",
      "====================\n",
      "Drop columns with same values\n",
      "\n",
      "data_appt_info shape (1047176, 47)\n",
      "\n",
      "Finish dropping\n",
      "====================\n",
      "Create features\n",
      "\n",
      " Patients age\n",
      "\n",
      " Month Date Week Hour\n",
      "\n",
      " Two type Duration\n",
      "========================================\n",
      "Start preprocessing data about treatment information\n",
      "====================\n",
      "Get data from mysql\n",
      "The shape of data_radiation (722368, 11)\n",
      "The shape of data_radiationhstry (2742710, 12)\n",
      "The shape of data_plan (148421, 13)\n",
      "The shape of data_course (58360, 11)\n",
      "The shape of data_patient (51062, 2)\n",
      "Drop columns\n",
      "\n",
      "Finish getting data\n",
      "====================\n",
      "Start merging data\n",
      "\n",
      "Process data_radiation\n",
      "The shape of data_radiation (243540, 10)\n",
      "\n",
      "Process data_radiationhstry\n",
      "The shape of data_radiationhstry (592728, 11)\n",
      "\n",
      "Merge data_radiation and data_radiationhstry\n",
      "The shape of ra_rh (507969, 19)\n",
      "\n",
      "Merge data_plan and ra_rh\n",
      "The shape of ra_rh_pl (507969, 29)\n",
      "\n",
      "Merge data_course and data_patient\n",
      "The shape of co_pa (58348, 10)\n",
      "\n",
      "Merge co_pa and ra_rh_pl\n",
      "The shape of data_treat_info (507863, 37)\n",
      "====================\n",
      "Drop columns with same values\n",
      "\n",
      "data_treat_info shape (507863, 33)\n",
      "\n",
      "Finish dropping\n",
      "====================\n",
      "Create features\n",
      "========================================\n",
      "Start data preprocess to generate x y\n",
      "====================\n",
      "Merge data_appt_info, data_treat_info\n",
      "\n",
      "The shape of data is (608291, 20)\n",
      "\n",
      "Finish merging\n",
      "====================\n",
      "Preprocess numberical data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finish preprocessing\n",
      "====================\n",
      "Preprocess categorical data\n",
      "\n",
      "Encoding features\n",
      "\n",
      "Finish preprocessing\n",
      "====================\n",
      "Concat data_num and data_cate\n",
      "\n",
      "Finish concating\n",
      "====================\n",
      "Get train set and test set\n",
      "\n",
      "The number of train set is 547461\n",
      "The number of test set is 60830\n",
      "Finish getting\n"
     ]
    }
   ],
   "source": [
    "data_appt_info = preprocess_appt_info()\n",
    "data_treat_info = preprocess_treat_info()\n",
    "data_display, data, train_x, train_y, test_x, test_y = data_preprocess_xy(data_appt_info, data_treat_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_display.to_csv(lightgbm_data_path + 'data_display.csv')\n",
    "# data.to_csv(lightgbm_data_path + 'data.csv')\n",
    "# train_x.to_csv(lightgbm_data_path + 'train_x.csv')\n",
    "# train_y.to_csv(lightgbm_data_path + 'train_y.csv')\n",
    "# test_x.to_csv(lightgbm_data_path + 'test_x.csv')\n",
    "# test_y.to_csv(lightgbm_data_path + 'test_y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mae 更好一点，rmse 和mse 可能会存在过拟合问题，测试结果的误差都比较大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Train model\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l1: 6.55939\n",
      "[400]\tvalid_0's l1: 6.29708\n",
      "[600]\tvalid_0's l1: 6.15965\n",
      "[800]\tvalid_0's l1: 6.05134\n",
      "[1000]\tvalid_0's l1: 5.98191\n",
      "[1200]\tvalid_0's l1: 5.93486\n",
      "[1400]\tvalid_0's l1: 5.89987\n",
      "[1600]\tvalid_0's l1: 5.87641\n",
      "[1800]\tvalid_0's l1: 5.85687\n",
      "[2000]\tvalid_0's l1: 5.83135\n",
      "[2200]\tvalid_0's l1: 5.80418\n",
      "[2400]\tvalid_0's l1: 5.78813\n",
      "[2600]\tvalid_0's l1: 5.77129\n",
      "[2800]\tvalid_0's l1: 5.75551\n",
      "[3000]\tvalid_0's l1: 5.71991\n",
      "[3200]\tvalid_0's l1: 5.70512\n",
      "[3400]\tvalid_0's l1: 5.69738\n",
      "[3600]\tvalid_0's l1: 5.68465\n",
      "[3800]\tvalid_0's l1: 5.67845\n",
      "[4000]\tvalid_0's l1: 5.66096\n",
      "[4200]\tvalid_0's l1: 5.63485\n",
      "[4400]\tvalid_0's l1: 5.62205\n",
      "[4600]\tvalid_0's l1: 5.61221\n",
      "[4800]\tvalid_0's l1: 5.59852\n",
      "[5000]\tvalid_0's l1: 5.59121\n",
      "[5200]\tvalid_0's l1: 5.58449\n",
      "[5400]\tvalid_0's l1: 5.57831\n",
      "[5600]\tvalid_0's l1: 5.57254\n",
      "[5800]\tvalid_0's l1: 5.56748\n",
      "[6000]\tvalid_0's l1: 5.56128\n",
      "[6200]\tvalid_0's l1: 5.55631\n",
      "[6400]\tvalid_0's l1: 5.55145\n",
      "[6600]\tvalid_0's l1: 5.54411\n",
      "[6800]\tvalid_0's l1: 5.53846\n",
      "[7000]\tvalid_0's l1: 5.53343\n",
      "[7200]\tvalid_0's l1: 5.52911\n",
      "[7400]\tvalid_0's l1: 5.52563\n",
      "[7600]\tvalid_0's l1: 5.52164\n",
      "[7800]\tvalid_0's l1: 5.5143\n",
      "[8000]\tvalid_0's l1: 5.51004\n",
      "[8200]\tvalid_0's l1: 5.50554\n",
      "[8400]\tvalid_0's l1: 5.50231\n",
      "[8600]\tvalid_0's l1: 5.49263\n",
      "[8800]\tvalid_0's l1: 5.48817\n",
      "[9000]\tvalid_0's l1: 5.48092\n",
      "[9200]\tvalid_0's l1: 5.47724\n",
      "[9400]\tvalid_0's l1: 5.47418\n",
      "[9600]\tvalid_0's l1: 5.47095\n",
      "[9800]\tvalid_0's l1: 5.46746\n",
      "[10000]\tvalid_0's l1: 5.46397\n",
      "[10200]\tvalid_0's l1: 5.46016\n",
      "[10400]\tvalid_0's l1: 5.45706\n",
      "[10600]\tvalid_0's l1: 5.45469\n",
      "[10800]\tvalid_0's l1: 5.45242\n",
      "[11000]\tvalid_0's l1: 5.4501\n",
      "[11200]\tvalid_0's l1: 5.44793\n",
      "[11400]\tvalid_0's l1: 5.42983\n",
      "[11600]\tvalid_0's l1: 5.4241\n",
      "[11800]\tvalid_0's l1: 5.42038\n",
      "[12000]\tvalid_0's l1: 5.41702\n",
      "[12200]\tvalid_0's l1: 5.41311\n",
      "[12400]\tvalid_0's l1: 5.40918\n",
      "[12600]\tvalid_0's l1: 5.40148\n",
      "[12800]\tvalid_0's l1: 5.39829\n",
      "[13000]\tvalid_0's l1: 5.39677\n",
      "[13200]\tvalid_0's l1: 5.39492\n",
      "[13400]\tvalid_0's l1: 5.39245\n",
      "[13600]\tvalid_0's l1: 5.39042\n",
      "[13800]\tvalid_0's l1: 5.38869\n",
      "[14000]\tvalid_0's l1: 5.38698\n",
      "[14200]\tvalid_0's l1: 5.38376\n",
      "[14400]\tvalid_0's l1: 5.3817\n",
      "[14600]\tvalid_0's l1: 5.37463\n",
      "[14800]\tvalid_0's l1: 5.37218\n",
      "[15000]\tvalid_0's l1: 5.37062\n",
      "[15200]\tvalid_0's l1: 5.36958\n",
      "[15400]\tvalid_0's l1: 5.36766\n",
      "[15600]\tvalid_0's l1: 5.36715\n",
      "[15800]\tvalid_0's l1: 5.36495\n",
      "[16000]\tvalid_0's l1: 5.36259\n",
      "[16200]\tvalid_0's l1: 5.36132\n",
      "[16400]\tvalid_0's l1: 5.36038\n",
      "[16600]\tvalid_0's l1: 5.35943\n",
      "[16800]\tvalid_0's l1: 5.35703\n",
      "[17000]\tvalid_0's l1: 5.35277\n",
      "[17200]\tvalid_0's l1: 5.35003\n",
      "[17400]\tvalid_0's l1: 5.34231\n",
      "[17600]\tvalid_0's l1: 5.34135\n",
      "[17800]\tvalid_0's l1: 5.34013\n",
      "[18000]\tvalid_0's l1: 5.33866\n",
      "[18200]\tvalid_0's l1: 5.33681\n",
      "[18400]\tvalid_0's l1: 5.33579\n",
      "[18600]\tvalid_0's l1: 5.32917\n",
      "[18800]\tvalid_0's l1: 5.32563\n",
      "[19000]\tvalid_0's l1: 5.32095\n",
      "[19200]\tvalid_0's l1: 5.31791\n",
      "[19400]\tvalid_0's l1: 5.31678\n",
      "[19600]\tvalid_0's l1: 5.31549\n",
      "[19800]\tvalid_0's l1: 5.3143\n",
      "[20000]\tvalid_0's l1: 5.31316\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20000]\tvalid_0's l1: 5.31316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.05, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=20000, n_jobs=-1, num_leaves=31, objective='mae',\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建模型，训练模型\n",
    "print('='*40)\n",
    "print('Train model')\n",
    "gbm = lgb.LGBMRegressor(objective='mae', num_leaves=31, learning_rate=0.05,\n",
    "                        n_estimators = 20000)\n",
    "gbm.fit(\n",
    "    train_x[: int(train_x.shape[0] * 0.9)], train_y[: int(train_y.shape[0] * 0.9)],\n",
    "    eval_set=[(train_x[int(train_x.shape[0] * 0.1):], train_y[int(train_y.shape[0] * 0.1):])],\n",
    "    eval_metric='mae',\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.709379339104933\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "pred_y = gbm.predict(test_x, num_iteration = gbm.best_iteration_)\n",
    "\n",
    "mae = MAE(pred_y, np.array(test_y.Actual_duration.tolist()))\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Train model\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's rmse: 19.775\n",
      "[400]\tvalid_0's rmse: 16.7929\n",
      "[600]\tvalid_0's rmse: 14.9787\n",
      "[800]\tvalid_0's rmse: 13.5861\n",
      "[1000]\tvalid_0's rmse: 12.7968\n",
      "[1200]\tvalid_0's rmse: 12.1218\n",
      "[1400]\tvalid_0's rmse: 11.5732\n",
      "[1600]\tvalid_0's rmse: 11.0291\n",
      "[1800]\tvalid_0's rmse: 10.609\n",
      "[2000]\tvalid_0's rmse: 10.3116\n",
      "[2200]\tvalid_0's rmse: 10.0337\n",
      "[2400]\tvalid_0's rmse: 9.70387\n",
      "[2600]\tvalid_0's rmse: 9.48474\n",
      "[2800]\tvalid_0's rmse: 9.28432\n",
      "[3000]\tvalid_0's rmse: 9.08105\n",
      "[3200]\tvalid_0's rmse: 8.90199\n",
      "[3400]\tvalid_0's rmse: 8.71331\n",
      "[3600]\tvalid_0's rmse: 8.51646\n",
      "[3800]\tvalid_0's rmse: 8.36673\n",
      "[4000]\tvalid_0's rmse: 8.23148\n",
      "[4200]\tvalid_0's rmse: 8.09924\n",
      "[4400]\tvalid_0's rmse: 7.98462\n",
      "[4600]\tvalid_0's rmse: 7.86878\n",
      "[4800]\tvalid_0's rmse: 7.7719\n",
      "[5000]\tvalid_0's rmse: 7.66164\n",
      "[5200]\tvalid_0's rmse: 7.56456\n",
      "[5400]\tvalid_0's rmse: 7.46278\n",
      "[5600]\tvalid_0's rmse: 7.36127\n",
      "[5800]\tvalid_0's rmse: 7.27494\n",
      "[6000]\tvalid_0's rmse: 7.17981\n",
      "[6200]\tvalid_0's rmse: 7.10826\n",
      "[6400]\tvalid_0's rmse: 7.03582\n",
      "[6600]\tvalid_0's rmse: 6.95674\n",
      "[6800]\tvalid_0's rmse: 6.87951\n",
      "[7000]\tvalid_0's rmse: 6.8106\n",
      "[7200]\tvalid_0's rmse: 6.75071\n",
      "[7400]\tvalid_0's rmse: 6.69023\n",
      "[7600]\tvalid_0's rmse: 6.63523\n",
      "[7800]\tvalid_0's rmse: 6.58871\n",
      "[8000]\tvalid_0's rmse: 6.50888\n",
      "[8200]\tvalid_0's rmse: 6.4508\n",
      "[8400]\tvalid_0's rmse: 6.40778\n",
      "[8600]\tvalid_0's rmse: 6.35595\n",
      "[8800]\tvalid_0's rmse: 6.30291\n",
      "[9000]\tvalid_0's rmse: 6.26294\n",
      "[9200]\tvalid_0's rmse: 6.222\n",
      "[9400]\tvalid_0's rmse: 6.18425\n",
      "[9600]\tvalid_0's rmse: 6.14267\n",
      "[9800]\tvalid_0's rmse: 6.10804\n",
      "[10000]\tvalid_0's rmse: 6.07491\n",
      "[10200]\tvalid_0's rmse: 6.0325\n",
      "[10400]\tvalid_0's rmse: 5.98665\n",
      "[10600]\tvalid_0's rmse: 5.94038\n",
      "[10800]\tvalid_0's rmse: 5.90434\n",
      "[11000]\tvalid_0's rmse: 5.86561\n",
      "[11200]\tvalid_0's rmse: 5.82507\n",
      "[11400]\tvalid_0's rmse: 5.79294\n",
      "[11600]\tvalid_0's rmse: 5.75759\n",
      "[11800]\tvalid_0's rmse: 5.71714\n",
      "[12000]\tvalid_0's rmse: 5.69153\n",
      "[12200]\tvalid_0's rmse: 5.65967\n",
      "[12400]\tvalid_0's rmse: 5.62073\n",
      "[12600]\tvalid_0's rmse: 5.5888\n",
      "[12800]\tvalid_0's rmse: 5.5551\n",
      "[13000]\tvalid_0's rmse: 5.52594\n",
      "[13200]\tvalid_0's rmse: 5.49521\n",
      "[13400]\tvalid_0's rmse: 5.46165\n",
      "[13600]\tvalid_0's rmse: 5.43211\n",
      "[13800]\tvalid_0's rmse: 5.40318\n",
      "[14000]\tvalid_0's rmse: 5.37547\n",
      "[14200]\tvalid_0's rmse: 5.35086\n",
      "[14400]\tvalid_0's rmse: 5.32244\n",
      "[14600]\tvalid_0's rmse: 5.29447\n",
      "[14800]\tvalid_0's rmse: 5.26959\n",
      "[15000]\tvalid_0's rmse: 5.24253\n",
      "[15200]\tvalid_0's rmse: 5.21421\n",
      "[15400]\tvalid_0's rmse: 5.18672\n",
      "[15600]\tvalid_0's rmse: 5.16104\n",
      "[15800]\tvalid_0's rmse: 5.13778\n",
      "[16000]\tvalid_0's rmse: 5.11638\n",
      "[16200]\tvalid_0's rmse: 5.09085\n",
      "[16400]\tvalid_0's rmse: 5.07142\n",
      "[16600]\tvalid_0's rmse: 5.04668\n",
      "[16800]\tvalid_0's rmse: 5.02299\n",
      "[17000]\tvalid_0's rmse: 4.99696\n",
      "[17200]\tvalid_0's rmse: 4.97471\n",
      "[17400]\tvalid_0's rmse: 4.95354\n",
      "[17600]\tvalid_0's rmse: 4.93653\n",
      "[17800]\tvalid_0's rmse: 4.91684\n",
      "[18000]\tvalid_0's rmse: 4.89866\n",
      "[18200]\tvalid_0's rmse: 4.87981\n",
      "[18400]\tvalid_0's rmse: 4.86058\n",
      "[18600]\tvalid_0's rmse: 4.84379\n",
      "[18800]\tvalid_0's rmse: 4.82414\n",
      "[19000]\tvalid_0's rmse: 4.80726\n",
      "[19200]\tvalid_0's rmse: 4.79076\n",
      "[19400]\tvalid_0's rmse: 4.77191\n",
      "[19600]\tvalid_0's rmse: 4.75234\n",
      "[19800]\tvalid_0's rmse: 4.73492\n",
      "[20000]\tvalid_0's rmse: 4.71742\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20000]\tvalid_0's rmse: 4.71742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.05, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=20000, n_jobs=-1, num_leaves=31, objective='rmse',\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建模型，训练模型\n",
    "print('='*40)\n",
    "print('Train model')\n",
    "gbm_rmse = lgb.LGBMRegressor(objective='rmse', num_leaves=31, learning_rate=0.05,\n",
    "                        n_estimators = 20000)\n",
    "gbm_rmse.fit(\n",
    "    train_x[: int(train_x.shape[0] * 0.9)], train_y[: int(train_y.shape[0] * 0.9)],\n",
    "    eval_set=[(train_x[int(train_x.shape[0] * 0.1):], train_y[int(train_y.shape[0] * 0.1):])],\n",
    "    eval_metric='rmse',\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.636110119719364\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "pred_y = gbm_rmse.predict(test_x, num_iteration = gbm_rmse.best_iteration_)\n",
    "\n",
    "rmse = RMSE(pred_y, np.array(test_y.Actual_duration.tolist()))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于分类变量，需要显示原始变量值的话，就需要加载display\n",
    "train_x_display = data_display.iloc[: int(0.9 * data.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
