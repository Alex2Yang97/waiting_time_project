{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PATH = 'D:\\\\waiting time\\\\我的代码\\\\TrainData\\\\'\n",
    "# 加载数据\n",
    "def pickleLoad(filename, local_path = LOCAL_PATH):\n",
    "    with open(local_path + filename, 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        data = u.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "#def pickleDump(data_to_pickle, filename, local_path = LOCAL_PATH):\n",
    " #   \"\"\" Function to write data to pickle format. \"\"\"\n",
    "  #  pickle.dump(data_to_pickle, open(local_path + filename, \"wb+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字符串转换成日期\n",
    "def StrToDate(date_str):\n",
    "    date = datetime.datetime.strptime(date_str,'%Y-%m-%d').date()\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较两次相邻的duration所间隔的天数\n",
    "def compareDate(date1, date2, ndays = 7):\n",
    "    interval = (date2 - date1).days\n",
    "    if interval >= ndays:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Old and New data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\waiting time\\\\我的代码\\\\raw_treatments1.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bbff62be4844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Load Old and New data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlocal_path1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:\\\\waiting time\\\\我的代码\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mraw_treatments_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickleLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'raw_treatments1.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_path1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlocal_path2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:\\\\waiting time\\\\data\\\\Regression\\\\all_data\\\\'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mraw_treatments_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickleLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PreInputData.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_path2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d6e8b7ec47de>\u001b[0m in \u001b[0;36mpickleLoad\u001b[1;34m(filename, local_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 加载数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpickleLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLOCAL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Unpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'latin1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\waiting time\\\\我的代码\\\\raw_treatments1.p'"
     ]
    }
   ],
   "source": [
    "# 加载之前保存的旧数据和新数据，然后合并两部分数据\n",
    "print (\"Load Old and New data...\")\n",
    "local_path1 = 'D:\\\\waiting time\\\\我的代码\\\\'\n",
    "raw_treatments_new = pickleLoad('raw_treatments1.p', local_path1)\n",
    "local_path2 = 'D:\\\\waiting time\\\\data\\\\Regression\\\\all_data\\\\'\n",
    "raw_treatments_old = pickleLoad('PreInputData.p', local_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_treatments_new_dict = {}\n",
    "for pat in raw_treatments_new.keys():\n",
    "    appt_dict = {}\n",
    "    for cou in raw_treatments_new[pat].keys():\n",
    "        for fra in raw_treatments_new[pat][cou].keys():\n",
    "            for date in raw_treatments_new[pat][cou][fra].keys():\n",
    "                appt_new = raw_treatments_new[pat][cou][fra][date]\n",
    "                appt_new['course'] = cou\n",
    "                appt_new['fraction'] = fra\n",
    "                appt_dict[date] = appt_new\n",
    "    raw_treatments_new_dict[pat] = appt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_treatments_old_dict = {}\n",
    "for pat in raw_treatments_old.keys():\n",
    "    appt_dict = {}\n",
    "    for cou in raw_treatments_old[pat].keys():\n",
    "        for fra in raw_treatments_old[pat][cou].keys():\n",
    "            for date in raw_treatments_old[pat][cou][fra].keys():\n",
    "                appt_new = raw_treatments_old[pat][cou][fra][date]\n",
    "                appt_new['course'] = cou\n",
    "                appt_new['fraction'] = fra\n",
    "                appt_dict[date] = appt_new\n",
    "    raw_treatments_old_dict[pat] = appt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pat in raw_treatments_old_dict.keys():\n",
    "    if pat in raw_treatments_new_dict.keys():\n",
    "        for date in raw_treatments_old_dict[pat].keys():\n",
    "            if date in raw_treatments_new_dict[pat].keys():\n",
    "                pass\n",
    "            else:\n",
    "                raw_treatments_new_dict[pat][date] = raw_treatments_old_dict[pat][date]\n",
    "                \n",
    "    else:\n",
    "        raw_treatments_new_dict[pat] = raw_treatments_old_dict[pat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Save raw_treatments_new_dict\")\n",
    "pickleDump(raw_treatments_new_dict, 'raw_treatments_new_dict.p')\n",
    "\n",
    "print (\"\\nLoad raw_treatments_new_dict\")\n",
    "raw_treatments_new_dict = pickleLoad('raw_treatments_new_dict.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_ser, course, fraction, date\n",
    "# 没有用的特征 'radiation_ser', 'prev_duration', 'avg_duration', 'allocated'\n",
    "# 不确定的 tot_mu, tot_mucoeff\n",
    "# 数量太多，不使用 tot_mu, tot_mucoeff, plan, radiation_id\n",
    "feature_nonnum = ['course',\n",
    "                  'fraction',\n",
    "                  'therapist',\n",
    "                  'resource',\n",
    "                  'diagnosis',\n",
    "                  'oncologist',\n",
    "                  'appt_day_of_week',\n",
    "                  'appt_month',\n",
    "                  'appt_hour',\n",
    "                  'orientation',\n",
    "                  'gender',]\n",
    "\n",
    "feature_num = ['num_of_fields',\n",
    "               'images_taken',\n",
    "               'image_duration',\n",
    "               'age',]\n",
    "\n",
    "feature_duration = ['duration']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "def getAveAndStd(raw_treatments_new_dict, name):\n",
    "    feature = []\n",
    "    for pat in raw_treatments_new_dict.keys():\n",
    "        for date in raw_treatments_new_dict[pat].keys():\n",
    "            text = raw_treatments_new_dict[pat][date]\n",
    "            feature.append(text[name])\n",
    "    \n",
    "    feature = np.array(feature)\n",
    "    feature_ave = np.mean(feature)\n",
    "    feature_std = np.std(feature)\n",
    "    \n",
    "    return feature_ave, feature_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到每个特征对应的平均值和标准差\n",
    "def GetFeatureAveStdDict(raw_treatments_new_dict, feature_num = feature_num):\n",
    "    feature_ave_std_dict = {}\n",
    "    for name in feature_num:\n",
    "        print('\\n\\n', name)\n",
    "        feature_ave, feature_std = getAveAndStd(raw_treatments_new_dict, name)\n",
    "        feature_ave_std_dict[name + '_ave'] = feature_ave\n",
    "        feature_ave_std_dict[name + '_std'] = feature_std\n",
    "    return feature_ave_std_dict\n",
    "\n",
    "feature_ave_std_dict_new = GetFeatureAveStdDict(raw_treatments_new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得数据的one-hot字典，同时进行数据归一化\n",
    "def GetFeatureDictAndNorm(raw_treatments_new_dict, feature_ave_std_dict_new):\n",
    "    print('\\nGet Nonnum One-hot Dictionary and Num normalize...')\n",
    "    print (\"\\nLoad feature_onehot_dict\")\n",
    "    feature_onehot_dict = pickleLoad('feature_onehot_dict.p')\n",
    "    \n",
    "    raw_treatments_new_list = []\n",
    "    pat_ser_set = []\n",
    "    pat_ser_list = []\n",
    "    date_list = []\n",
    "    for pat in raw_treatments_new_dict.keys():\n",
    "        pat_ser_set.append(pat)\n",
    "        for date in sorted(raw_treatments_new_dict[pat]):\n",
    "            pat_ser_list.append(pat)\n",
    "            date_list.append(StrToDate(date))\n",
    "            treat_list = [pat, StrToDate(date)]\n",
    "            text = raw_treatments_new_dict[pat][date]\n",
    "            \n",
    "            # 数值\n",
    "            for name in feature_num:\n",
    "                value = text[name]\n",
    "                value = value - feature_ave_std_dict_new[name + '_ave']\n",
    "                value = value / feature_ave_std_dict_new[name + '_std']\n",
    "                treat_list.append(value)\n",
    "                \n",
    "                # 非数值\n",
    "            for name in feature_nonnum:\n",
    "                treat_list.append(text[name])\n",
    "            duration = sum(text['duration'])\n",
    "            treat_list.append([duration])\n",
    "            raw_treatments_new_list.append(treat_list)\n",
    "    \n",
    "    print('\\nThe length of the raw_treatments_new_list: ', len(raw_treatments_new_list))        \n",
    "    print('\\nShow some samples:')\n",
    "    for i in raw_treatments_new_list[:2]:\n",
    "        print(i)\n",
    "        print('\\n')\n",
    "        \n",
    "    print('\\nSave raw_treatments_new_list')\n",
    "    pickleDump(raw_treatments_new_list, 'raw_treatments_new_list.p')    \n",
    "    \n",
    "    \n",
    "    return feature_onehot_dict, raw_treatments_new_list, pat_ser_set, pat_ser_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_onehot_dict, raw_treatments_new_list, pat_ser_set, pat_ser_list = GetFeatureDictAndNorm(raw_treatments_new_dict, feature_ave_std_dict_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nLoad raw_treatments_new_list\")\n",
    "raw_treatments_new_list = pickleLoad('raw_treatments_new_list.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将不同患者，以及时间间隔过大的数据进行分割\n",
    "def GetCutList(raw_treatments_new_list, pat_ser_list, pat_ser_set):\n",
    "    raw_treatments_new_cut = []\n",
    "    raw_treatments_new_cut_len = []\n",
    "    #raw_len = []\n",
    "    for i in range(len(pat_ser_set) - 1):\n",
    "        start = pat_ser_list.index(pat_ser_set[i])\n",
    "        end = pat_ser_list.index(pat_ser_set[i + 1])\n",
    "        \n",
    "        one_pat = raw_treatments_new_list[start: end]\n",
    "#        one_pat.sort(key = lambda one_pat: one_pat[3])\n",
    "        \n",
    "        flag = 0\n",
    "        for appt_i in range(len(one_pat) - 1):\n",
    "            date1 = one_pat[appt_i][1]\n",
    "            date2 = one_pat[appt_i + 1][1]\n",
    "            if compareDate(date1, date2):\n",
    "                raw_treatments_new_cut.append(one_pat[flag: appt_i + 1])\n",
    "                raw_treatments_new_cut_len.append(len(one_pat[flag: appt_i + 1]))\n",
    "                flag = appt_i + 1\n",
    "        if flag != len(one_pat):\n",
    "            raw_treatments_new_cut.append(one_pat[flag: ])\n",
    "            raw_treatments_new_cut_len.append(len(one_pat[flag: appt_i + 1]))\n",
    "    \n",
    "    max_idx = raw_treatments_new_cut_len.index(max(raw_treatments_new_cut_len))\n",
    "    pat_treatment = raw_treatments_new_cut[max_idx]\n",
    "    \n",
    "    print('\\nSave raw_treatments_new_cut')\n",
    "    pickleDump(raw_treatments_new_cut, 'raw_treatments_new_cut.p')\n",
    "    \n",
    "    return raw_treatments_new_cut, pat_treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_treatments_new_cut, pat_treatment = GetCutList(raw_treatments_new_list, pat_ser_list, pat_ser_set)\n",
    "TIME_STEPS = len(pat_treatment)\n",
    "print (\"\\nLoad raw_treatments_new_cut\")\n",
    "raw_treatments_new_cut = pickleLoad('raw_treatments_new_cut.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把第一次治疗的数据提出来\n",
    "# 之前使用了第一次的duration当作sample_sequence的一个特征\n",
    "# 现在区分第一次的duration，sample_sequence从第三次治疗开始\n",
    "def SepFirst(raw_treatments_new_cut):\n",
    "    feature = len(raw_treatments_new_cut[0][0])\n",
    "    sample_first_new = []\n",
    "    sample_sequence_new = []\n",
    "    \n",
    "    \n",
    "    print('Get sample_first and sample_sequence')\n",
    "    for i in range(len(raw_treatments_new_cut)):\n",
    "        pat = raw_treatments_new_cut[i]\n",
    "        if len(pat) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            if pat[0][7] == 1:\n",
    "                sample_first_new.append(pat[0])\n",
    "                prev_duration = []\n",
    "                for i in range(len(pat) - 1):\n",
    "                    prev_duration.append(pat[i][-1])\n",
    "                \n",
    "                pat_sequence = pat[1:]\n",
    "                for i in range(len(pat_sequence)):\n",
    "                    pat_sequence[i].append(prev_duration[i])\n",
    "                sample_sequence_new.append(pat_sequence)\n",
    "                \n",
    "            else:\n",
    "                sample_first_new.append([0] * feature)\n",
    "                prev_duration = [[0]]\n",
    "                for i in range(len(pat) - 1):\n",
    "                    prev_duration.append(pat[i][-1])\n",
    "                \n",
    "                pat_sequence = pat\n",
    "                for i in range(len(pat_sequence)):\n",
    "                    pat_sequence[i].append(prev_duration[i])\n",
    "                sample_sequence_new.append(pat_sequence)\n",
    "    \n",
    "    print('\\nSave sample_first_new')\n",
    "    pickleDump(sample_first_new, 'sample_first_new.p')\n",
    "    print('\\nSave sample_sequence_new')\n",
    "    pickleDump(sample_sequence_new, 'sample_sequence_new.p')\n",
    "    \n",
    "    return sample_first_new, sample_sequence_new\n",
    "\n",
    "sample_first_new, sample_sequence_new = SepFirst(raw_treatments_new_cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二次的治疗，prev_duration是第一次的治疗时间\n",
    "# 因此，将第二次的治疗和sequence的治疗分开\n",
    "def CutSecondAppt(sample_first_new, sample_sequence_new):\n",
    "    count_ind = []\n",
    "    sample_first_new_cut = []\n",
    "    sample_second = []\n",
    "    sample_sequence_new_cut = []\n",
    "    \n",
    "    for i in range(len(sample_sequence_new)):\n",
    "        pat_first = sample_first_new[i]\n",
    "        pat_sequence = sample_sequence_new[i]\n",
    "        if len(pat_sequence) > 1:\n",
    "            sample_first_new_cut.append(pat_first)\n",
    "            pat_second = pat_sequence[0]\n",
    "            sample_second.append(pat_second)\n",
    "            pat_cut = pat_sequence[1:]\n",
    "            sample_sequence_new_cut.append(pat_cut)\n",
    "            count_ind.append(i)\n",
    "    \n",
    "    return count_ind, sample_first_new_cut, sample_second, sample_sequence_new_cut\n",
    "\n",
    "_, sample_first_new_cut, sample_second, sample_sequence_new_cut = CutSecondAppt(sample_first_new, sample_sequence_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prev_bench_mark(sample_sequence_new_cut):\n",
    "    cost_list = []\n",
    "    \n",
    "    for i in range(len(sample_sequence_new_cut)):\n",
    "        # 得到一个病人 patient\n",
    "        pat_sequence = sample_sequence_new_cut[i]\n",
    "        \n",
    "        for j in range(len(pat_sequence)):\n",
    "            appt = pat_sequence[j]\n",
    "            prev_duration = appt[-2]\n",
    "            now_duration = appt[-1]\n",
    "            cost = abs(prev_duration[0] - now_duration[0])\n",
    "            cost_list.append(cost)\n",
    "    \n",
    "    bench_mark = sum(cost_list) / len(cost_list)    \n",
    "    return bench_mark, cost_list\n",
    "\n",
    "bench_mark, _ = get_prev_bench_mark(sample_sequence_new_cut)\n",
    "# 5.9724657627689925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Onehot(term, onehot_dict):\n",
    "    result = np.zeros((len(onehot_dict), ))\n",
    "    \n",
    "    if type(term) == list:\n",
    "        for i in term:\n",
    "            if i != 0:\n",
    "                result[onehot_dict.index(i)] = 1\n",
    "    else:\n",
    "        if term != 0:\n",
    "            result[onehot_dict.index(term)] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombineOnehot(appt, FEATURE, F_onehot = feature_onehot_dict):\n",
    "    result = np.zeros((FEATURE, ))\n",
    "    \n",
    "    if FEATURE == 746:\n",
    "        if type(appt[5]) == int:\n",
    "            result[:5] = appt[2:6] + appt[-1]\n",
    "        else:\n",
    "            appt_num = appt[2:5] + appt[5].tolist() + appt[-1]\n",
    "            result[:5] = appt_num\n",
    "        start_ind = 5\n",
    "    else:\n",
    "        if type(appt[5]) == int:\n",
    "            result[:4] = appt[2:6]\n",
    "        else:\n",
    "            appt_num = appt[2:5] + appt[5].tolist()\n",
    "            result[:4] = appt_num\n",
    "        start_ind = 4\n",
    "        \n",
    "    appt_nonnum = appt[6:17]\n",
    "    for i in range(len(appt_nonnum)):\n",
    "        name = feature_nonnum[i] + '_onehot'\n",
    "        onehot_dict = F_onehot[name]\n",
    "        result[start_ind: start_ind + len(onehot_dict)] = Onehot(appt_nonnum[i], onehot_dict)\n",
    "        start_ind = start_ind + len(onehot_dict)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用one-hot编码，根据最长的特征向量，其他的特征向量填补零\n",
    "# 将所有数据补成等长的\n",
    "def GetPadding(pat, j, TIME_STEPS, FEATURE):\n",
    "    one_sample = np.zeros((TIME_STEPS, FEATURE))\n",
    "    \n",
    "    for n in range(len(pat[: j + 1])):\n",
    "        appt = pat[n]\n",
    "        re = CombineOnehot(appt, FEATURE)\n",
    "        one_sample[n, :] = re\n",
    "    \n",
    "    return one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoricalFeature(appt):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_dense(train_sample, count_pre, BATCH, FEATURE):\n",
    "    count = 0\n",
    "    n = 0\n",
    "    \n",
    "    batch_train = np.zeros((BATCH, FEATURE))\n",
    "    batch_y = np.zeros((BATCH, ))\n",
    "    \n",
    "    for i in range(len(train_sample)):\n",
    "        # onehot\n",
    "        if count < count_pre:\n",
    "            count = count + 1\n",
    "        elif count >= count_pre and (count - count_pre) < BATCH:\n",
    "            # 得到appt\n",
    "            appt = train_sample[i]\n",
    "            batch_train[n, :] = CombineOnehot(appt, FEATURE)\n",
    "            batch_y[n] = appt[-1][0]\n",
    "            n = n + 1\n",
    "            count = count + 1\n",
    "        else:\n",
    "            break\n",
    "#    print('count is ', count)\n",
    "    state = np.random.get_state()    \n",
    "    np.random.shuffle(batch_train)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(batch_y)\n",
    "    \n",
    "    return batch_train, batch_y, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_sequence(train_sequence_new, count_pre, BATCH, TIME_STEPS, FEATURE):    \n",
    "    count = 0\n",
    "    n = 0\n",
    "    \n",
    "    batch_train_sequence = np.zeros((BATCH, TIME_STEPS, FEATURE))\n",
    "    batch_y_sequence = np.zeros((BATCH, ))\n",
    "    \n",
    "    for i in range(len(train_sequence_new)):\n",
    "        # 得到一个病人 patient\n",
    "        pat_sequence = train_sequence_new[i]\n",
    "        \n",
    "        for j in range(len(pat_sequence)):\n",
    "            appt = pat_sequence[j]\n",
    "            \n",
    "            if count < count_pre:\n",
    "                count = count + 1\n",
    "            elif count >= count_pre and (count - count_pre) < BATCH:\n",
    "                batch_y_sequence[n] = appt[-2][0]\n",
    "                one_sample = GetPadding(pat_sequence, j, TIME_STEPS, FEATURE)\n",
    "                batch_train_sequence[n, :, :] = one_sample\n",
    "                n = n + 1\n",
    "                count = count + 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "#    print('count is ', count)\n",
    "    state = np.random.get_state()    \n",
    "    np.random.shuffle(batch_train_sequence)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(batch_y_sequence)\n",
    "    \n",
    "    return batch_train_sequence, batch_y_sequence, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dense_model(FEATURE):\n",
    "    model_Dense = Sequential()\n",
    "    model_Dense.add(layers.Dense(\n",
    "            activation='relu',\n",
    "            batch_input_shape=(None, FEATURE),\n",
    "            output_dim = 128,\n",
    "            ))\n",
    "    \n",
    "    model_Dense.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model_Dense.add(layers.Dense(\n",
    "            activation='relu',\n",
    "            output_dim = 32,\n",
    "            ))\n",
    "    \n",
    "    model_Dense.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model_Dense.add(layers.Dense(\n",
    "            activation='relu',\n",
    "            output_dim = 1,\n",
    "            ))\n",
    "\n",
    "    model_Dense.summary()\n",
    "    model_Dense.compile(optimizer='rmsprop', loss='mae')\n",
    "    \n",
    "    return model_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_model(TIME_STEPS, FEATURE):\n",
    "    # 后续治疗，fraction number > 1\n",
    "    model_sequence = Sequential()\n",
    "    model_sequence.add(layers.GRU(\n",
    "            batch_input_shape = (None , TIME_STEPS, FEATURE),\n",
    "            output_dim = 128,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.5,\n",
    "            return_sequences=True,\n",
    "            ))\n",
    "    \n",
    "    model_sequence.add(layers.GRU(\n",
    "            output_dim = 32,\n",
    "            dropout=0.1,\n",
    "            recurrent_dropout=0.5,\n",
    "            ))\n",
    "    # stateful = True 本次batch的参数返回到下一次的训练中\n",
    "    \n",
    "    model_sequence.add(layers.Dense(1))\n",
    "    \n",
    "    model_sequence.compile(\n",
    "            optimizer = 'rmsprop',\n",
    "            loss = 'mae'\n",
    "            )\n",
    "    \n",
    "    model_sequence.summary()\n",
    "    return model_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingCost(STEP_PLOT, COST_PLOT, BATCH, STEPS, plot_step, NAME):\n",
    "    plt.plot(STEP_PLOT, COST_PLOT, color='blue', label='training loss')\n",
    "    plt.plot(STEP_PLOT, [4.6] * len(COST_PLOT), color='red', label='Random Forest loss')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.ylabel('Train Cost (%d Steps-%d Batch)'%(plot_step, BATCH))\n",
    "    plt.xlabel('Steps')\n",
    "    plt.title('training_cost_%s-%dtime_steps- %dbatch'%(NAME, STEPS, BATCH))\n",
    "    plt.grid()\n",
    "    plt.rcParams['savefig.dpi'] = 300  # 图片像素\n",
    "    plt.rcParams['figure.dpi'] = 300 # 分辨率\n",
    "    plt.rcParams['figure.figsize'] = (15,15)  # 尺寸\n",
    "    plt.savefig(LOCAL_PATH + \"training_cost_%s-%dtime_steps-%dbatch.png\"%(NAME, STEPS, BATCH), format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRealPredict(result_sequence, batch_y_test_sequence, BATCH, BATCH_TEST, STEPS, plot_step, NAME):\n",
    "    plt.plot(batch_y_test_sequence, color='red', label='real')\n",
    "    plt.plot(result_sequence, color='blue', label='predict')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.ylabel('Train Cost (%d Steps-%d Batch)/minutes'%(plot_step, BATCH_TEST))\n",
    "    plt.xlabel('Appointments')\n",
    "    plt.title('Real-Prediction_%s-%dSteps-%dbatch'%(NAME, STEPS, BATCH))\n",
    "    plt.grid()\n",
    "    plt.rcParams['savefig.dpi'] = 300  # 图片像素\n",
    "    plt.rcParams['figure.dpi'] = 300 # 分辨率\n",
    "    plt.rcParams['figure.figsize'] = (15,15)  # 尺寸\n",
    "    plt.savefig(LOCAL_PATH + \"Real-Prediction_%s-%dtime_steps-%dbatch.png\"%(NAME, STEPS, BATCH), format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = raw_treatments_new_list[:230000]\n",
    "test_sample = raw_treatments_new_list[230000:]\n",
    "print('\\ntrain_samplet length is ', len(train_sample))\n",
    "print('\\ntest_sample length is ', len(test_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DENSE = 10\n",
    "FEATURE_DENSE = 745\n",
    "print('\\nTraining ------------')\n",
    "COUNT_PRE_DENSE = 0\n",
    "COST_DENSE = 0\n",
    "COST_PLOT_DENSE = []\n",
    "STEP_PLOT_DENSE = []\n",
    "plot_step_dense = 100\n",
    "print('\\nbatch_train shape is : (%d, %d)'%(BATCH_DENSE, FEATURE_DENSE))\n",
    "print('\\nbatch_y shape is : (%d, )'%BATCH_DENSE)\n",
    "STEPS_DENSE = len(train_sample) // BATCH_DENSE\n",
    "print(STEPS_DENSE)\n",
    "\n",
    "model_dense = Dense_model(FEATURE_DENSE)\n",
    "STEPS_DENSE = 23000\n",
    "for step in range(STEPS_DENSE):\n",
    "    #    print('COUNT_PRE_first:', COUNT_PRE_first)\n",
    "    batch_train, batch_y, COUNT_PRE_DENSE = get_batch_dense(train_sample, COUNT_PRE_DENSE, BATCH_DENSE, FEATURE_DENSE)\n",
    "    cost_Dense = model_dense.train_on_batch(batch_train, batch_y)\n",
    "    \n",
    "    if (step + 1) % plot_step_dense == 0:\n",
    "#        print('\\n%d round average train cost is %f'%((step + 1), cost_Dense))\n",
    "        print('\\n%d round average train cost is %f'%((step + 1), (COST_DENSE / plot_step_dense)))\n",
    "        COST_PLOT_DENSE.append(COST_DENSE / plot_step_dense)\n",
    "        STEP_PLOT_DENSE.append(step + 1)\n",
    "        COST_DENSE = 0\n",
    "    else:\n",
    "        COST_DENSE = COST_DENSE + cost_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_PRE_DENSE_TEST = 0\n",
    "BATCH_DENSE_TEST = 200\n",
    "results_dense = []\n",
    "targets_dense = []\n",
    "for step in range(1):\n",
    "    print('Test %d round ', (step + 1))\n",
    "    \n",
    "    batch_train_dense, batch_y_dense, COUNT_PRE_DENSE_TEST = get_batch_dense(train_sample, COUNT_PRE_DENSE_TEST, BATCH_DENSE_TEST, FEATURE_DENSE)\n",
    "    result_dense = model_dense.predict_on_batch(batch_train_dense)\n",
    "#    targets_dense.append(batch_y_dense)\n",
    "#    results_dense.append(result_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'all_dense'\n",
    "plotTrainingCost(STEP_PLOT_DENSE, COST_PLOT_DENSE, BATCH_DENSE, STEPS_DENSE, plot_step_dense, NAME)\n",
    "plotRealPredict(result_dense, batch_y_dense, BATCH_DENSE, BATCH_DENSE_TEST, STEPS_DENSE, plot_step_dense, NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dense = Dense_model(FEATURE_DENSE)\n",
    "plot_model(model_dense, to_file = LOCAL_PATH + 'model_dense.png', show_shapes = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_first_new = sample_first_new[:15000]\n",
    "#test_first_new = sample_first_new[15000:]\n",
    "#\n",
    "#train_sequence_new = sample_sequence_new[:15000]\n",
    "#test_sequence_new = sample_sequence_new[15000:]\n",
    "\n",
    "train_first_new_cut = sample_first_new_cut[:15000]\n",
    "test_first_new_cut = sample_first_new_cut[15000:]\n",
    "\n",
    "train_sequence_new_cut = sample_sequence_new_cut[:15000]\n",
    "test_sequence_new_cut = sample_sequence_new_cut[15000:]\n",
    "\n",
    "def CountSampleInSequence(train_sequence_new_cut):\n",
    "    total_sample_number = 0\n",
    "    for pat in train_sequence_new_cut:\n",
    "        for appt in pat:\n",
    "            total_sample_number = total_sample_number + 1\n",
    "    return total_sample_number\n",
    "\n",
    "total_sample_number = CountSampleInSequence(train_sequence_new)\n",
    "# 239992\n",
    "total_sample_number_cut = CountSampleInSequence(train_sequence_new_cut)\n",
    "# 229862\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 100\n",
    "STEPS = total_sample_number // BATCH\n",
    "STEPS_cut = total_sample_number_cut // BATCH\n",
    "print(STEPS_cut)\n",
    "\n",
    "print('\\nTraining ------------')\n",
    "COUNT_PRE_sequence = 0\n",
    "COST_sequence = 0\n",
    "TIME_STEPS = 50\n",
    "FEATURE = 746\n",
    "plot_step = 50\n",
    "COST_PLOT = []\n",
    "STEP_PLOT = []\n",
    "model_sequence = GRU_model(TIME_STEPS, FEATURE)\n",
    "print('\\nbatch_train_first shape is : (%d, %d)'%(BATCH, FEATURE))\n",
    "print('\\nbatch_train_first shape is : (%d, )'%BATCH)\n",
    "print('\\nbatch_train_sequence shape is : (%d, %d, %d)'%(BATCH, TIME_STEPS, FEATURE))\n",
    "print('\\nbatch_train_sequence shape is : (%d, )'%BATCH)\n",
    "#STEPS = len(raw_treatments_new_list) // BATCH\n",
    "#STEPS = 2350\n",
    "STEPS = 2250\n",
    "#check_batch_y_sequence = []\n",
    "for step in range(STEPS):\n",
    "    batch_train_sequence, batch_y_sequence, COUNT_PRE_sequence = get_batch_sequence(train_sequence_new_cut, COUNT_PRE_sequence, BATCH, TIME_STEPS, FEATURE)\n",
    "#    check_batch_y_sequence.append(batch_y_sequence)\n",
    "    cost_sequence = model_sequence.train_on_batch(batch_train_sequence, batch_y_sequence)\n",
    "    \n",
    "    if (step + 1) % plot_step == 0:\n",
    "        print('%d round average train cost is %f'%((step + 1), (COST_sequence / plot_step)))\n",
    "        COST_PLOT.append(COST_sequence / plot_step)\n",
    "        STEP_PLOT.append(step + 1)\n",
    "        COST_sequence = 0\n",
    "    else:\n",
    "        COST_sequence = COST_sequence + cost_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sequence = GRU_model(TIME_STEPS, FEATURE)\n",
    "plot_model(model_sequence, to_file = LOCAL_PATH + 'model_sequence.png', show_shapes = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"Save raw_treatments_new_dict\")\n",
    "#pickleDump(raw_treatments_new_dict, 'raw_treatments_new_dict.p')\n",
    "#print (\"Save raw_treatments_new_dict\")\n",
    "#pickleDump(raw_treatments_new_dict, 'raw_treatments_new_dict.p')\n",
    "#print (\"Save raw_treatments_new_dict\")\n",
    "#pickleDump(raw_treatments_new_dict, 'raw_treatments_new_dict.p')\n",
    "\n",
    "\n",
    "#model_sequence.save('model_sequence_cut_second_dropout_%dbatch_%dsteps_NoCut.h5'%(BATCH, STEPS))\n",
    "\n",
    "#from keras.models import load_model\n",
    "#model_sequence = load_model('model_sequence_cut_second_%dbatch_%dsteps_NoCut.h5'%(BATCH, STEPS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_better(COST_PLOT):\n",
    "    count_better = []\n",
    "    for i in COST_PLOT:\n",
    "        if i < 4.6:\n",
    "            count_better.append(i)\n",
    "    return count_better\n",
    "\n",
    "count_better = count_better(COST_PLOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_PRE = 0\n",
    "BATCH_TEST = 200\n",
    "results_sequence = []\n",
    "targets_sequence = []\n",
    "for step in range(1):\n",
    "    print('Test %d round ', (step + 1))\n",
    "    \n",
    "    batch_test_sequence, batch_y_test_sequence, COUNT_PRE = get_batch_sequence(test_sequence_new_cut, COUNT_PRE, BATCH_TEST, TIME_STEPS, FEATURE)\n",
    "    result_sequence = model_sequence.predict(batch_test_sequence)\n",
    "#    results_sequence.append(result_sequence)\n",
    "#    targets_sequence.append(batch_y_test_sequence)\n",
    "\n",
    "NAME = 'cut_dropout'\n",
    "plotTrainingCost(STEP_PLOT, COST_PLOT, BATCH, STEPS, plot_step, NAME)\n",
    "plotRealPredict(result_sequence, batch_y_test_sequence, BATCH, BATCH_TEST, STEPS, TIME_STEPS, plot_step, NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
